<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Google Earth Engine for OSINT - 6&nbsp; Vectors and Tables</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./F6.html" rel="next">
<link href="./F4.html" rel="prev">
<link href="./favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-RK9ZLZQ6GL"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-RK9ZLZQ6GL', { 'anonymize_ip': true});
</script>


</head>

<body class="nav-sidebar floating">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Vectors and Tables</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./logo_white.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Google Earth Engine for OSINT</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/oballinger/GEE_OSINT/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="" title="Download" id="sidebar-tool-dropdown-0" class="sidebar-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi bi-download"></i></a>
    <ul class="dropdown-menu" aria-labelledby="sidebar-tool-dropdown-0">
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="./Google-Earth-Engine-for-OSINT.pdf">
            <i class="bi bi-bi-file-pdf pe-1"></i>
          Download PDF
          </a>
        </li>
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="./Google-Earth-Engine-for-OSINT.epub">
            <i class="bi bi-bi-journal pe-1"></i>
          Download ePub
          </a>
        </li>
    </ul>
    <a href="" title="Share" id="sidebar-tool-dropdown-1" class="sidebar-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi bi-share"></i></a>
    <ul class="dropdown-menu" aria-labelledby="sidebar-tool-dropdown-1">
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
            <i class="bi bi-bi-twitter pe-1"></i>
          Twitter
          </a>
        </li>
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
            <i class="bi bi-bi-facebook pe-1"></i>
          Facebook
          </a>
        </li>
    </ul>
  <a href="" class="quarto-color-scheme-toggle sidebar-tool" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Introduction</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Learning</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Remote Sensing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Acquisition</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./F1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./F2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Interpreting Images</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./F4.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Interpreting Image Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./F5.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Vectors and Tables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./F6.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advanced Topics</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Case Studies</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lights.html" class="sidebar-item-text sidebar-link">War at Night</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./refineries.html" class="sidebar-item-text sidebar-link">Refinery Detection</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ships.html" class="sidebar-item-text sidebar-link">Refinery Detection</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#exploring-vectors" id="toc-exploring-vectors" class="nav-link active" data-scroll-target="#exploring-vectors"><span class="toc-section-number">7</span>  Exploring Vectors</a>
  <ul class="collapse">
  <li><a href="#using-geometry-tools-to-create-features-in-earth-engine" id="toc-using-geometry-tools-to-create-features-in-earth-engine" class="nav-link" data-scroll-target="#using-geometry-tools-to-create-features-in-earth-engine"><span class="toc-section-number">7.1</span>  Using&nbsp;Geometry Tools to Create Features in Earth Engine</a></li>
  <li><a href="#loading-existing-features-and-feature-collections-in-earth-engine" id="toc-loading-existing-features-and-feature-collections-in-earth-engine" class="nav-link" data-scroll-target="#loading-existing-features-and-feature-collections-in-earth-engine"><span class="toc-section-number">7.2</span>  Loading&nbsp;Existing Features&nbsp;and Feature Collections in Earth Engine</a></li>
  <li><a href="#importing-features-into-earth-engine" id="toc-importing-features-into-earth-engine" class="nav-link" data-scroll-target="#importing-features-into-earth-engine"><span class="toc-section-number">7.3</span>  Importing Features into Earth Engine</a>
  <ul class="collapse">
  <li><a href="#find-a-spatial-dataset-of-san-francisco-neighborhoods" id="toc-find-a-spatial-dataset-of-san-francisco-neighborhoods" class="nav-link" data-scroll-target="#find-a-spatial-dataset-of-san-francisco-neighborhoods"><span class="toc-section-number">7.3.1</span>  Find a Spatial Dataset of San Francisco Neighborhoods</a></li>
  <li><a href="#upload-sf-neighborhoods-file-as-an-asset" id="toc-upload-sf-neighborhoods-file-as-an-asset" class="nav-link" data-scroll-target="#upload-sf-neighborhoods-file-as-an-asset"><span class="toc-section-number">7.3.2</span>  Upload SF Neighborhoods File as an Asset</a></li>
  <li><a href="#select-files-and-name-asset" id="toc-select-files-and-name-asset" class="nav-link" data-scroll-target="#select-files-and-name-asset"><span class="toc-section-number">7.3.3</span>  Select Files and Name Asset</a></li>
  </ul></li>
  <li><a href="#filtering-feature-collections-by-attributes" id="toc-filtering-feature-collections-by-attributes" class="nav-link" data-scroll-target="#filtering-feature-collections-by-attributes"><span class="toc-section-number">7.4</span>  Filtering Feature Collections by Attributes</a>
  <ul class="collapse">
  <li><a href="#filter-by-geometry-of-another-feature" id="toc-filter-by-geometry-of-another-feature" class="nav-link" data-scroll-target="#filter-by-geometry-of-another-feature"><span class="toc-section-number">7.4.1</span>  Filter by Geometry of Another Feature</a></li>
  <li><a href="#filter-by-feature-attribute-properties" id="toc-filter-by-feature-attribute-properties" class="nav-link" data-scroll-target="#filter-by-feature-attribute-properties"><span class="toc-section-number">7.4.2</span>  Filter by Feature (Attribute) Properties</a></li>
  <li><a href="#print-feature-attribute-properties-to-console" id="toc-print-feature-attribute-properties-to-console" class="nav-link" data-scroll-target="#print-feature-attribute-properties-to-console"><span class="toc-section-number">7.4.3</span>  Print Feature (Attribute) Properties to Console</a></li>
  </ul></li>
  <li><a href="#reducing-images-using-feature-geometry" id="toc-reducing-images-using-feature-geometry" class="nav-link" data-scroll-target="#reducing-images-using-feature-geometry"><span class="toc-section-number">7.5</span>  Reducing Images Using Feature Geometry</a>
  <ul class="collapse">
  <li><a href="#create-an-ndvi-image" id="toc-create-an-ndvi-image" class="nav-link" data-scroll-target="#create-an-ndvi-image"><span class="toc-section-number">7.5.1</span>  Create an NDVI Image</a></li>
  <li><a href="#clip-the-ndvi-image-to-the-blocks-near-usf" id="toc-clip-the-ndvi-image-to-the-blocks-near-usf" class="nav-link" data-scroll-target="#clip-the-ndvi-image-to-the-blocks-near-usf"><span class="toc-section-number">7.5.2</span>  Clip the NDVI Image to the Blocks Near USF</a></li>
  <li><a href="#compute-ndvi-statistics-by-block" id="toc-compute-ndvi-statistics-by-block" class="nav-link" data-scroll-target="#compute-ndvi-statistics-by-block"><span class="toc-section-number">7.5.3</span>  Compute NDVI Statistics by Block</a></li>
  <li><a href="#export-table-of-ndvi-data-by-block-from-earth-engine-to-google-drive" id="toc-export-table-of-ndvi-data-by-block-from-earth-engine-to-google-drive" class="nav-link" data-scroll-target="#export-table-of-ndvi-data-by-block-from-earth-engine-to-google-drive"><span class="toc-section-number">7.5.4</span>  Export Table of NDVI Data by Block from Earth Engine to Google Drive</a></li>
  </ul></li>
  <li><a href="#identifying-the-block-in-the-neighborhood-surrounding-usf-with-the-highest-ndvi" id="toc-identifying-the-block-in-the-neighborhood-surrounding-usf-with-the-highest-ndvi" class="nav-link" data-scroll-target="#identifying-the-block-in-the-neighborhood-surrounding-usf-with-the-highest-ndvi"><span class="toc-section-number">7.6</span>  Identifying the Block in the Neighborhood Surrounding USF with the Highest NDVI</a></li>
  <li><a href="#synthesis" id="toc-synthesis" class="nav-link" data-scroll-target="#synthesis">Synthesis</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  <li><a href="#rastervector-conversions" id="toc-rastervector-conversions" class="nav-link" data-scroll-target="#rastervector-conversions"><span class="toc-section-number">8</span>  Raster/Vector Conversions</a>
  <ul class="collapse">
  <li><a href="#raster-to-vector-conversion" id="toc-raster-to-vector-conversion" class="nav-link" data-scroll-target="#raster-to-vector-conversion"><span class="toc-section-number">8.1</span>  Raster to Vector Conversion&nbsp;</a>
  <ul class="collapse">
  <li><a href="#raster-to-polygons" id="toc-raster-to-polygons" class="nav-link" data-scroll-target="#raster-to-polygons"><span class="toc-section-number">8.1.1</span>  Raster to Polygons</a></li>
  <li><a href="#raster-to-points" id="toc-raster-to-points" class="nav-link" data-scroll-target="#raster-to-points"><span class="toc-section-number">8.1.2</span>  Raster to Points</a></li>
  <li><a href="#raster-properties-to-vector-fields" id="toc-raster-properties-to-vector-fields" class="nav-link" data-scroll-target="#raster-properties-to-vector-fields"><span class="toc-section-number">8.1.3</span>  Raster Properties to Vector Fields</a></li>
  </ul></li>
  <li><a href="#vector-to-raster-conversion" id="toc-vector-to-raster-conversion" class="nav-link" data-scroll-target="#vector-to-raster-conversion"><span class="toc-section-number">8.2</span>  Vector-to-Raster Conversion</a>
  <ul class="collapse">
  <li><a href="#polygons-to-a-mask" id="toc-polygons-to-a-mask" class="nav-link" data-scroll-target="#polygons-to-a-mask"><span class="toc-section-number">8.2.1</span>  Polygons to a Mask</a></li>
  <li><a href="#a-more-complex-example" id="toc-a-more-complex-example" class="nav-link" data-scroll-target="#a-more-complex-example"><span class="toc-section-number">8.2.2</span>  A More Complex Example</a></li>
  </ul></li>
  <li><a href="#synthesis-1" id="toc-synthesis-1" class="nav-link" data-scroll-target="#synthesis-1">Synthesis</a></li>
  <li><a href="#conclusion-1" id="toc-conclusion-1" class="nav-link" data-scroll-target="#conclusion-1">Conclusion</a></li>
  </ul></li>
  <li><a href="#zonal-statistics" id="toc-zonal-statistics" class="nav-link" data-scroll-target="#zonal-statistics"><span class="toc-section-number">9</span>  Zonal Statistics</a>
  <ul class="collapse">
  <li><a href="#introduction-to-theory" id="toc-introduction-to-theory" class="nav-link" data-scroll-target="#introduction-to-theory"><span class="toc-section-number">9.1</span>  Introduction to Theory&nbsp;</a></li>
  <li><a href="#functions" id="toc-functions" class="nav-link" data-scroll-target="#functions"><span class="toc-section-number">9.2</span>  Functions</a>
  <ul class="collapse">
  <li><a href="#function-bufferpointsradius-bounds" id="toc-function-bufferpointsradius-bounds" class="nav-link" data-scroll-target="#function-bufferpointsradius-bounds"><span class="toc-section-number">9.2.1</span>  Function: bufferPoints(radius, bounds)</a></li>
  <li><a href="#function-zonalstatsfc-params" id="toc-function-zonalstatsfc-params" class="nav-link" data-scroll-target="#function-zonalstatsfc-params"><span class="toc-section-number">9.2.2</span>  Function: zonalStats(fc, params)</a></li>
  </ul></li>
  <li><a href="#point-collection-creation" id="toc-point-collection-creation" class="nav-link" data-scroll-target="#point-collection-creation"><span class="toc-section-number">9.3</span>  Point Collection Creation</a></li>
  <li><a href="#neighborhood-statistic-examples" id="toc-neighborhood-statistic-examples" class="nav-link" data-scroll-target="#neighborhood-statistic-examples"><span class="toc-section-number">9.4</span>  Neighborhood Statistic Examples</a>
  <ul class="collapse">
  <li><a href="#topographic-variables" id="toc-topographic-variables" class="nav-link" data-scroll-target="#topographic-variables"><span class="toc-section-number">9.4.1</span>  Topographic Variables</a></li>
  <li><a href="#modis-time-series" id="toc-modis-time-series" class="nav-link" data-scroll-target="#modis-time-series"><span class="toc-section-number">9.4.2</span>  MODIS Time Series</a></li>
  <li><a href="#landsat-time-series" id="toc-landsat-time-series" class="nav-link" data-scroll-target="#landsat-time-series"><span class="toc-section-number">9.4.3</span>  Landsat Time Series</a></li>
  </ul></li>
  <li><a href="#additional-notes" id="toc-additional-notes" class="nav-link" data-scroll-target="#additional-notes"><span class="toc-section-number">9.5</span>  Additional Notes</a>
  <ul class="collapse">
  <li><a href="#weighted-versus-unweighted-region-reduction" id="toc-weighted-versus-unweighted-region-reduction" class="nav-link" data-scroll-target="#weighted-versus-unweighted-region-reduction"><span class="toc-section-number">9.5.1</span>  Weighted Versus Unweighted Region Reduction</a></li>
  <li><a href="#copy-properties-to-computed-images" id="toc-copy-properties-to-computed-images" class="nav-link" data-scroll-target="#copy-properties-to-computed-images"><span class="toc-section-number">9.5.2</span>  Copy Properties to Computed Images</a></li>
  <li><a href="#understanding-which-pixels-are-included-in-polygon-statistics" id="toc-understanding-which-pixels-are-included-in-polygon-statistics" class="nav-link" data-scroll-target="#understanding-which-pixels-are-included-in-polygon-statistics"><span class="toc-section-number">9.5.3</span>  Understanding Which Pixels are Included in Polygon Statistics</a></li>
  </ul></li>
  <li><a href="#synthesis-2" id="toc-synthesis-2" class="nav-link" data-scroll-target="#synthesis-2">Synthesis</a></li>
  <li><a href="#conclusion-2" id="toc-conclusion-2" class="nav-link" data-scroll-target="#conclusion-2">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  <li><a href="#advanced-vector-operations" id="toc-advanced-vector-operations" class="nav-link" data-scroll-target="#advanced-vector-operations"><span class="toc-section-number">10</span>  Advanced Vector Operations</a>
  <ul class="collapse">
  <li><a href="#visualizing-feature-collections" id="toc-visualizing-feature-collections" class="nav-link" data-scroll-target="#visualizing-feature-collections"><span class="toc-section-number">10.1</span>  Visualizing Feature Collections</a>
  <ul class="collapse">
  <li><a href="#creating-a-choropleth-map" id="toc-creating-a-choropleth-map" class="nav-link" data-scroll-target="#creating-a-choropleth-map"><span class="toc-section-number">10.1.1</span>  Creating a Choropleth Map</a></li>
  <li><a href="#creating-a-categorical-map" id="toc-creating-a-categorical-map" class="nav-link" data-scroll-target="#creating-a-categorical-map"><span class="toc-section-number">10.1.2</span>  Creating a Categorical Map</a></li>
  </ul></li>
  <li><a href="#joins-with-feature-collections" id="toc-joins-with-feature-collections" class="nav-link" data-scroll-target="#joins-with-feature-collections"><span class="toc-section-number">10.2</span>  Joins with Feature Collections</a>
  <ul class="collapse">
  <li><a href="#selecting-by-location" id="toc-selecting-by-location" class="nav-link" data-scroll-target="#selecting-by-location"><span class="toc-section-number">10.2.1</span>  Selecting by Location</a></li>
  <li><a href="#spatial-joins" id="toc-spatial-joins" class="nav-link" data-scroll-target="#spatial-joins"><span class="toc-section-number">10.2.2</span>  Spatial Joins</a></li>
  </ul></li>
  <li><a href="#synthesis-3" id="toc-synthesis-3" class="nav-link" data-scroll-target="#synthesis-3">Synthesis</a></li>
  <li><a href="#conclusion-3" id="toc-conclusion-3" class="nav-link" data-scroll-target="#conclusion-3">Conclusion</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/oballinger/GEE_OSINT/edit/main/F5.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Vectors and Tables</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In addition to raster data processing, Earth Engine supports a rich set of vector processing tools. This Part introduces you to the vector framework in Earth Engine, shows you how to create and to import your vector data, and how to combine vector and raster data for analyses.</p>
<section id="exploring-vectors" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Exploring Vectors</h1>
<p>::: {.callout-tip} # Chapter Information</p>
<section id="author" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="author">Author</h2>
<p>AJ Purdy, Ellen Brock, David Saah</p>
<p>Overview</p>
<p>In this chapter, you will learn about features and feature collections and how to use them in conjunction with images and image collections in Earth Engine. Maps are useful for understanding spatial patterns, but scientists often need to extract statistics to answer a question. For example, you may make a false-color composite&nbsp;showing which areas of San Francisco are more “green”—i.e., have more healthy vegetation—than others, but you will likely not be able to directly determine which block in a neighborhood is the most green. This tutorial will demonstrate how to do just that by utilizing vectors.</p>
<p>As described in Chap. F4.0, an important way to summarize and simplify data in Earth Engine is through the use of reducers. Reducers operating across space were used in Chap. F3.0, for example, to enable image regression between bands. More generally, chapters in Part F3 and Part F4 used reducers mostly to summarize the values across bands or images on a pixel-by-pixel basis. What if you wanted to summarize information within the confines of given spatial elements- for example, within a set of polygons? In this chapter, we will illustrate and explore Earth Engine’s method for doing that, which is through a reduceRegions&nbsp;call.</p>
<p>Learning Outcomes</p>
<ul>
<li>Uploading and working with a shapefile as an asset to use in Earth Engine.</li>
<li>Creating a new feature using the geometry tools.</li>
<li>Importing and filtering a feature collection in Earth Engine.</li>
<li>Using a feature to clip and reduce image values within a geometry.</li>
<li>Use reduceRegions&nbsp;to summarize an image in irregular neighborhoods.</li>
<li>Exporting calculated data to tables with Tasks.</li>
</ul>
<p>Assumes you know how to:</p>
<ul>
<li>Import images and image collections, filter, and visualize (Part F1).</li>
<li>Calculate and interpret vegetation indices (Chap. F2.0).</li>
<li>Use drawing tools to create points, lines, and polygons (Chap. F2.1).</li>
</ul>
<p>Introduction to Theory&nbsp;</p>
<p>In the world of geographic information systems (GIS), data is typically thought of in one of two basic data structures: raster and&nbsp;vector. In previous chapters, we have principally been focused on raster data—data using the remote sensing vocabulary of pixels, spatial resolution, images, and image collections. Working within the vector framework is also a crucial skill to master. If you don’t know much about GIS, you can find any number of online explainers of the distinctions between these data types, their strengths and limitations, and analyses using both data types. Being able to move fluidly between a raster conception and a vector conception of the world is powerful, and is facilitated with specialized functions and approaches in Earth Engine. &nbsp; &nbsp;</p>
<p>For our purposes, you can think of vector data as information represented as points (e.g., locations of sample sites), lines (e.g., railroad tracks), or polygons (e.g., the boundary of a national park or a neighborhood). Line data and polygon data are built up from points: for example, the latitude and longitude of the sample sites, the points along the curve of the railroad tracks, and the corners of the park that form its boundary. These points each have a highly specific location on Earth’s surface, and the vector data formed from them can be used for calculations with respect to other layers. As will be seen in this chapter, for example, a polygon can be used to identify which pixels in an image are contained within its borders. Point-based data have already been used in earlier chapters for filtering image collections by location (see Part F1), and can also be used to extract values from an image at a point or a set of points (see Chap. F5.2). Lines possess the dimension of length and have similar capabilities for filtering image collections and accessing their values along a transect. In addition to using polygons to summarize values within a boundary, they can be used for other, similar purposes—for example, to clip an image.</p>
<p>As you have seen, raster features in Earth Engine are stored as an Image&nbsp;or as part of an ImageCollection. Using a similar conceptual model, vector data in Earth Engine is stored as a Feature&nbsp;or as part of a FeatureCollection. Features and feature collections provide useful data to filter images and image collections by their location, clip images to a boundary, or statistically summarize the pixel values within a region.</p>
<p>In the following example, you will use features&nbsp;and feature collections to identify which city block near the University of San Francisco (USF) campus is the most green.</p>
</section>
<section id="using-geometry-tools-to-create-features-in-earth-engine" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="using-geometry-tools-to-create-features-in-earth-engine"><span class="header-section-number">7.1</span> Using&nbsp;Geometry Tools to Create Features in Earth Engine</h2>
<p>To demonstrate how geometry tools in Earth Engine work, let’s start by creating a point, and two polygons to represent different elements on the USF campus.</p>
<p>Click&nbsp;on the geometry&nbsp;tools in the top left of the Map&nbsp;pane&nbsp;and create a point feature. Place a new point where USF is located (see Fig. F5.0.1).</p>
<p><img src="F5/image54.png" class="img-fluid"></p>
<p>Fig. F5.0.1&nbsp;Location of the USF campus in San Francisco, California. Your first point should be in this vicinity. The red arrow points to the geometry tools.</p>
<p>Use Google Maps to search for “Harney Science Center” or “Lo Schiavo Center for Science.” Hover your mouse over the Geometry Imports&nbsp;to find the +new layer&nbsp;menu item and add a new layer&nbsp;to delineate the boundary of a building on campus.</p>
<p>Next, create another new layer to represent the entire campus as a polygon.</p>
<p>After you create these layers, rename the geometry imports at the top of your script. Name the layers usf_point, usf_building, and usf_campus. These names are used&nbsp;within the script shown in Fig. F5.0.2.</p>
<p><img src="F5/image10.png" class="img-fluid"></p>
<p>Fig. F5.0.2&nbsp;Rename the default variable names for each layer in the Imports&nbsp;section of the code at the top of your script</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F50a.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.&nbsp;</p>
</div>
</div>
</section>
<section id="loading-existing-features-and-feature-collections-in-earth-engine" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="loading-existing-features-and-feature-collections-in-earth-engine"><span class="header-section-number">7.2</span> Loading&nbsp;Existing Features&nbsp;and Feature Collections in Earth Engine</h2>
<p>If you wish to have the exact same geometry imports in this chapter for the rest of this exercise, begin this section using the code at the Code Checkpoint above.</p>
<p>Next, you will load a city block dataset to determine the amount of vegetation on&nbsp;blocks near USF. The code below imports an existing feature dataset in Earth Engine. The Topologically Integrated Geographic Encoding and Referencing (TIGER) boundaries are census-designated boundaries that are a useful resource when comparing socioeconomic and diversity metrics with environmental datasets in the United States.</p>
<p>// Import the Census Tiger Boundaries from GEE.<br>
var&nbsp;tiger = ee.FeatureCollection(‘TIGER/2010/Blocks’);</p>
<p>// Add the new feature collection to the map, but do not display.<br>
Map.addLayer(tiger, {&nbsp; &nbsp;‘color’: ‘black’}, ‘Tiger’, false);</p>
<p>You should now have the geometry for USF’s campus and a layer added to your map that is not visualized&nbsp;for census blocks across the United States. Next, we will use neighborhood data to spatially filter the TIGER feature collection for blocks near USF’s campus.</p>
</section>
<section id="importing-features-into-earth-engine" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="importing-features-into-earth-engine"><span class="header-section-number">7.3</span> Importing Features into Earth Engine</h2>
<p>There are many image collections loaded in Earth Engine, and they can cover a very large area that you might want to study. Borders can be quite intricate (for example, a detailed coastline), and fortunately there is no need for you to digitize the intricate boundary of a large geographic area. Instead, we will show how to find a spatial dataset online, download the data, and load this into Earth Engine as an asset for use.</p>
<section id="find-a-spatial-dataset-of-san-francisco-neighborhoods" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="find-a-spatial-dataset-of-san-francisco-neighborhoods"><span class="header-section-number">7.3.1</span> Find a Spatial Dataset of San Francisco Neighborhoods</h3>
<p>Use your internet searching skills to locate the “Analysis Neighborhoods” dataset covering San Francisco. This data might be located in a number of places, including DataSF, the City of San Francisco’s public-facing data repository.</p>
<p><img src="F5/image27.png" class="img-fluid"></p>
<p>Fig. F5.0.3&nbsp;DataSF website neighborhood shapefile to download</p>
<p>After you find the Analysis Neighborhoods layer, click Export&nbsp;and select&nbsp;Shapefile&nbsp;(Fig. F5.0.3). Keep track of where you save the zipped file, as we will load this into Earth Engine. Shapefiles contain vector-based data—points, lines, polygons—and include a number of files, such as the location information, attribute information, and others.</p>
<p>Extract the folder to your computer. When you open the folder, you will see that there are actually many files. The extensions (.shp, .dbf, .shx, .prj) all provide a different piece of information to display vector-based data. The .shp&nbsp;file provides data on the geometry. The .dbf&nbsp;file provides data about the attributes. The .shx&nbsp;file is an index file. Lastly, the .prj&nbsp;file describes the map projection of the coordinate information for the shapefile. You will need to load all four files to create a new feature asset in Earth Engine.</p>
</section>
<section id="upload-sf-neighborhoods-file-as-an-asset" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="upload-sf-neighborhoods-file-as-an-asset"><span class="header-section-number">7.3.2</span> Upload SF Neighborhoods File as an Asset</h3>
<p>Navigate to the Assets&nbsp;tab (near Scripts). Select New&nbsp;&gt; Table Upload&nbsp;&gt; Shape files&nbsp;(Fig. F5.0.4).</p>
<p><img src="F5/image52.png" class="img-fluid"></p>
<p>Fig. F5.0.4&nbsp;Import an asset as a zipped folder</p>
</section>
<section id="select-files-and-name-asset" class="level3" data-number="7.3.3">
<h3 data-number="7.3.3" class="anchored" data-anchor-id="select-files-and-name-asset"><span class="header-section-number">7.3.3</span> Select Files and Name Asset</h3>
<p>Click the Select&nbsp;button and then use the file navigator to select&nbsp;the component files of the shapefile structure (i.e., .shp, .dbf, .shx, and .prj) (Fig. F5.0.5). Assign an Asset&nbsp;Name&nbsp;so you can recognize this asset.</p>
<p><img src="F5/image43.png" class="img-fluid"></p>
<p>Fig. F5.0.5&nbsp;Select the four files extracted from the zipped folder. Make sure each file has the same name and that there are no spaces in the file names of the component files of the shapefile structure.</p>
<p>Uploading the asset may take a few minutes. The status of the upload is presented under the Tasks&nbsp;tab. After your asset has been successfully loaded, click on the asset in the Assets&nbsp;folder and find the collection ID. Copy this text and use it to import the file into your Earth Engine analysis.</p>
<p>Assign the asset to the table (collection) ID using the script below. Note that you will need to replace ‘path/to/your/asset/assetname’&nbsp;with the actual path copied in the previous step.</p>
<p>// Assign the feature collection to the variable sfNeighborhoods.<br>
var&nbsp;sfNeighborhoods = ee.FeatureCollection(&nbsp; &nbsp;‘path/to/your/asset/assetname’);</p>
<p>// Print the size of the feature collection.<br>
// (Answers the question how many features?)<br>
print(sfNeighborhoods.size());<br>
Map.addLayer(sfNeighborhoods, {&nbsp; &nbsp;‘color’: ‘blue’}, ‘sfNeighborhoods’);</p>
<p>Note that if you have any trouble with loading the FeatureCollection&nbsp;using the technique above, you can follow directions in the Checkpoint script below to use an existing asset loaded for this exercise.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F50b.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.</p>
</div>
</div>
</section>
</section>
<section id="filtering-feature-collections-by-attributes" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="filtering-feature-collections-by-attributes"><span class="header-section-number">7.4</span> Filtering Feature Collections by Attributes</h2>
<section id="filter-by-geometry-of-another-feature" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="filter-by-geometry-of-another-feature"><span class="header-section-number">7.4.1</span> Filter by Geometry of Another Feature</h3>
<p>First, let’s find the neighborhood associated with USF. Use the first point you created to find the neighborhood that intersects this point; filterBounds&nbsp;is the tool that does that, returning a filtered feature.</p>
<p>// Filter sfNeighborhoods by USF.<br>
var&nbsp;usfNeighborhood = sfNeighborhoods.filterBounds(usf_point);</p>
<p>Now, filter the blocks layer by USF’s neighborhood and visualize it on the map.</p>
<p>// Filter the Census blocks by the boundary of the neighborhood layer.<br>
var&nbsp;usfTiger = tiger.filterBounds(usfNeighborhood);<br>
Map.addLayer(usfTiger, {}, ‘usf_Tiger’);</p>
</section>
<section id="filter-by-feature-attribute-properties" class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="filter-by-feature-attribute-properties"><span class="header-section-number">7.4.2</span> Filter by Feature (Attribute) Properties</h3>
<p>In addition to filtering a FeatureCollection&nbsp;by the location of another feature, you can also filter it by its properties. First, let’s print the usfTiger&nbsp;variable to the Console&nbsp;and inspect the object.</p>
<p>print(usfTiger);</p>
<p>You can click on the feature collection name in the Console&nbsp;to uncover more information about the dataset. Click on the columns to learn about what attribute information is contained in this dataset. You will notice this feature collection contains information on both housing (‘housing10’) and population&nbsp;(‘pop10’).</p>
<p>Now you will filter for blocks with just the right amount of housing units. You don’t want it too dense, nor do you want too few neighbors.</p>
<p>Filter the blocks to have fewer than 250 housing units.</p>
<p>// Filter for census blocks by housing units.<br>
var&nbsp;housing10_l250 = usfTiger<br>
&nbsp; &nbsp;.filter(ee.Filter.lt(‘housing10’, 250));</p>
<p>Now filter the already-filtered blocks to have more than 50 housing units.</p>
<p>var&nbsp;housing10_g50_l250 = housing10_l250.filter(ee.Filter.gt(&nbsp; &nbsp;‘housing10’, 50));</p>
<p>Now, let’s visualize what this looks like.</p>
<p>Map.addLayer(housing10_g50_l250, {&nbsp; &nbsp;‘color’: ‘Magenta’}, ‘housing’);</p>
<p>We have combined spatial and attribute information to narrow the set to only those blocks that meet our criteria&nbsp;of having between 50 and 250 housing units.</p>
</section>
<section id="print-feature-attribute-properties-to-console" class="level3" data-number="7.4.3">
<h3 data-number="7.4.3" class="anchored" data-anchor-id="print-feature-attribute-properties-to-console"><span class="header-section-number">7.4.3</span> Print Feature (Attribute) Properties to Console</h3>
<p>We can print out attribute information about these features. The block of code below prints out the area&nbsp;of the resultant geometry in square meters.</p>
<p>var&nbsp;housing_area = housing10_g50_l250.geometry().area();<br>
print(‘housing_area:’, housing_area);</p>
<p>The next block of code reduces attribute information and prints out the mean of the housing10&nbsp;column.</p>
<p>var&nbsp;housing10_mean = usfTiger.reduceColumns({<br>
&nbsp; &nbsp;reducer: ee.Reducer.mean(),<br>
&nbsp; &nbsp;selectors: [‘housing10’]<br>
});</p>
<p>print(‘housing10_mean’, housing10_mean);</p>
<p>Both of the above sections of code provide meaningful information about each feature, but they do not tell us which block is the most green. The next section will address that question.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F50c.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.&nbsp;</p>
</div>
</div>
</section>
</section>
<section id="reducing-images-using-feature-geometry" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="reducing-images-using-feature-geometry"><span class="header-section-number">7.5</span> Reducing Images Using Feature Geometry</h2>
<p>Now that we have identified the blocks around USF’s campus that have the right housing density, let’s find which blocks are the greenest.</p>
<p>The Normalized Difference Vegetation Index (NDVI), presented in detail in&nbsp;Chap. F2.0, is often used to compare the greenness of pixels in different locations. Values on land range from 0 to 1, with values closer to 1 representing healthier and greener vegetation than values near 0.</p>
<section id="create-an-ndvi-image" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="create-an-ndvi-image"><span class="header-section-number">7.5.1</span> Create an NDVI Image</h3>
<p>The code below imports the Landsat 8 ImageCollection&nbsp;as landsat8. Then, the code filters for images in 2021. Lastly, the code sorts the images from 2021 to find the least cloudy day.</p>
<p>// Import the Landsat 8 TOA image collection.<br>
var&nbsp;landsat8 = ee.ImageCollection(‘LANDSAT/LC08/C02/T1_TOA’);</p>
<p>// Get the least cloudy image in 2015.<br>
var&nbsp;image = ee.Image(<br>
&nbsp; &nbsp;landsat8<br>
&nbsp; &nbsp;.filterBounds(usf_point)<br>
&nbsp; &nbsp;.filterDate(‘2015-01-01’, ‘2015-12-31’)<br>
&nbsp; &nbsp;.sort(‘CLOUD_COVER’)<br>
&nbsp; &nbsp;.first());</p>
<p>The next section of code assigns the near-infrared band (B5) to variable nir&nbsp;and assigns the red band (B4) to red. Then the bands are combined together to compute NDVI as (nir&nbsp;− red)/(nir&nbsp;+ red).</p>
<p>var&nbsp;nir = image.select(‘B5’);<br>
var&nbsp;red = image.select(‘B4’);<br>
var&nbsp;ndvi = nir.subtract(red).divide(nir.add(red)).rename(‘NDVI’);</p>
</section>
<section id="clip-the-ndvi-image-to-the-blocks-near-usf" class="level3" data-number="7.5.2">
<h3 data-number="7.5.2" class="anchored" data-anchor-id="clip-the-ndvi-image-to-the-blocks-near-usf"><span class="header-section-number">7.5.2</span> Clip the NDVI Image to the Blocks Near USF</h3>
<p>Next, you will clip the NDVI layer to only show NDVI over USF’s neighborhood.</p>
<p>The first section of code provides visualization settings.</p>
<p>var&nbsp;ndviParams = {<br>
&nbsp; &nbsp;min: -1,<br>
&nbsp; &nbsp;max: 1,<br>
&nbsp; &nbsp;palette: [‘blue’, ‘white’, ‘green’]<br>
};</p>
<p>The second block of code clips the image to our filtered housing layer.</p>
<p>var&nbsp;ndviUSFblocks = ndvi.clip(housing10_g50_l250);<br>
Map.addLayer(ndviUSFblocks, ndviParams, ‘NDVI image’);<br>
Map.centerObject(usf_point, 14);</p>
<p>The NDVI map for all of San Francisco is interesting, and shows variability across the region. Now, let’s compute mean&nbsp;NDVI values for each block&nbsp;of the city.</p>
</section>
<section id="compute-ndvi-statistics-by-block" class="level3" data-number="7.5.3">
<h3 data-number="7.5.3" class="anchored" data-anchor-id="compute-ndvi-statistics-by-block"><span class="header-section-number">7.5.3</span> Compute NDVI Statistics by Block</h3>
<p>The code below uses the clipped image ndviUSFblocks&nbsp;and computes the mean NDVI value within each boundary. The scale provides a spatial resolution for the mean values to be computed on.</p>
<p>// Reduce image by feature to compute a statistic e.g.&nbsp;mean, max, min etc.<br>
var&nbsp;ndviPerBlock = ndviUSFblocks.reduceRegions({<br>
&nbsp; &nbsp;collection: housing10_g50_l250,<br>
&nbsp; &nbsp;reducer: ee.Reducer.mean(),<br>
&nbsp; &nbsp;scale: 30,<br>
});</p>
<p>Now we’ll use Earth Engine to find out which block is greenest. &nbsp;</p>
</section>
<section id="export-table-of-ndvi-data-by-block-from-earth-engine-to-google-drive" class="level3" data-number="7.5.4">
<h3 data-number="7.5.4" class="anchored" data-anchor-id="export-table-of-ndvi-data-by-block-from-earth-engine-to-google-drive"><span class="header-section-number">7.5.4</span> Export Table of NDVI Data by Block from Earth Engine to Google Drive</h3>
<p>Just as we loaded a feature into Earth Engine, we can export information from Earth Engine. Here, we will export the NDVI data, summarized by block, from Earth Engine to a Google Drive space so that we can interpret it in a program like Google Sheets or Excel.</p>
<p>// Get a table of data out of Google Earth Engine.<br>
Export.table.toDrive({<br>
&nbsp; &nbsp;collection: ndviPerBlock,<br>
&nbsp; &nbsp;description: ‘NDVI_by_block_near_USF’<br>
});</p>
<p>When you run this code, you will notice that you have the Tasks&nbsp;tab highlighted on the top right of the Earth Engine Code Editor (Fig. F5.0.6). You will be prompted to name the directory when exporting the data.</p>
<p><img src="F5/image4.png" class="img-fluid"></p>
<p>Fig. F5.0.6&nbsp;Under the Tasks&nbsp;tab, select Run&nbsp;to initiate download</p>
<p>After you run the task, the file will be saved to your Google Drive. You have now brought a feature into Earth Engine and also exported data from Earth Engine.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F50d.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.&nbsp;</p>
</div>
</div>
</section>
</section>
<section id="identifying-the-block-in-the-neighborhood-surrounding-usf-with-the-highest-ndvi" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="identifying-the-block-in-the-neighborhood-surrounding-usf-with-the-highest-ndvi"><span class="header-section-number">7.6</span> Identifying the Block in the Neighborhood Surrounding USF with the Highest NDVI</h2>
<p>You are already familiar with filtering datasets by their attributes. Now you will sort a table and select the first element of the table.</p>
<p>ndviPerBlock = ndviPerBlock.select([‘blockid10’, ‘mean’]);<br>
print(‘ndviPerBlock’, ndviPerBlock);<br>
var&nbsp;ndviPerBlockSorted = ndviPerBlock.sort(‘mean’, false);<br>
var&nbsp;ndviPerBlockSortedFirst = ee.Feature(ndviPerBlock.sort(‘mean’,&nbsp; &nbsp; &nbsp; &nbsp;false) //Sort by NDVI mean in descending order.&nbsp; &nbsp;.first()); //Select the block with the highest NDVI.<br>
print(‘ndviPerBlockSortedFirst’, ndviPerBlockSortedFirst);</p>
<p>If you expand the feature of ndviPerBlockSortedFirst&nbsp;in the Console,&nbsp;you will be able to identify the blockid10&nbsp;value of the greenest block and the mean NDVI value for that area.</p>
<p>Another way to look at the data is by exporting the data to a table. Open the table using Google Sheets or Excel. You should see a column titled “mean.” Sort the mean column in descending order from highest NDVI to lowest NDVI, then use the blockid10&nbsp;attribute to filter our feature collection one last time and display the greenest block near USF.</p>
<p>// Now filter by block and show on map!<br>
var&nbsp;GreenHousing = usfTiger.filter(ee.Filter.eq(‘blockid10’,<br>
‘###’)); //&lt; Put your id here prepend a 0!<br>
Map.addLayer(GreenHousing, {&nbsp; &nbsp;‘color’: ‘yellow’}, ‘Green Housing!’);</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F50e.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.&nbsp;</p>
</div>
</div>
</section>
<section id="synthesis" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="synthesis">Synthesis</h2>
<p>Now it’s your turn to use both feature classes and to reduce data using a geographic boundary. Create a new script for an area of interest and accomplish the following assignments.</p>
<p>Assignment 1.&nbsp;Create a study area map zoomed to a certain feature class that you made.</p>
<p>Assignment 2.&nbsp;Filter one feature collection using feature properties.</p>
<p>Assignment 3.&nbsp;Filter one feature collection based on another feature’s location in space.</p>
<p>Assignment 4.&nbsp;Reduce one image to the geometry of a feature in some capacity; e.g., extract a mean value or a value at a point.</p>
</section>
<section id="conclusion" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this chapter, you learned how to import features into Earth Engine. In Sect. 1, you created new features using the geometry tools and loaded a feature from Earth Engine’s Data Catalog. In Sect. 2, you loaded a shapefile to an Earth Engine asset. In Sect. 3, you filtered feature collections based on their properties and locations. Finally, in Sects. 4 and 5, you used a feature collection to reduce an image, then exported the data from Earth Engine. Now you have all the tools you need to load, filter, and apply features to extract meaningful information from images using vector features in Earth Engine.</p>
</section>
</section>
<section id="rastervector-conversions" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Raster/Vector Conversions</h1>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Chapter Information
</div>
</div>
<div class="callout-body-container callout-body">
<section id="author-1" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="author-1">Author</h2>
<p>Keiko Nomura, Samuel Bowers</p>
</section>
<section id="overview" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>&nbsp;</p>
<p>The purpose of this chapter is to review methods of converting between raster and vector data formats, and to understand the circumstances in which this is useful. By way of example, this chapter focuses on topographic elevation and forest cover change in Colombia, but note that these are generic methods that can be applied in a wide variety of situations.</p>
</section>
<section id="learning-outcomes" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="learning-outcomes">Learning Outcomes</h2>
<ul>
<li>Understanding raster and vector data in Earth Engine and their differing properties.</li>
<li>Knowing how and why to convert from raster to vector.</li>
<li>Knowing how and why to convert from vector to raster.</li>
<li>Write a function and map&nbsp;it over a FeatureCollection.</li>
</ul>
</section>
<section id="assumes-you-know-how-to" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="assumes-you-know-how-to">Assumes you know how to:</h2>
<ul>
<li>Import images and image collections, filter, and visualize (Part F1).</li>
<li>Understand distinctions among&nbsp;Image, ImageCollection, Feature&nbsp;and FeatureCollection&nbsp;Earth Engine objects&nbsp;(Part F1, Part F2, Part F5).</li>
<li>Perform basic image analysis: select bands, compute indices, create masks (Part F2).</li>
<li>Perform image morphological operations (Chap. F3.2).</li>
<li>Understand the filter, map, reduce&nbsp;paradigm (Chap. F4.0).</li>
<li>Write a function and map&nbsp;it over an ImageCollection&nbsp;(Chap. F4.0).</li>
<li>Use reduceRegions&nbsp;to summarize an image in irregular shapes (Chap. F5.0).</li>
</ul>
</section>
</div>
</div>
<section id="introduction" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="introduction">Introduction</h2>
<p>Raster data consists of regularly spaced pixels arranged into rows and columns, familiar as the format of satellite images. Vector data contains geometry features (i.e., points, lines, and polygons) describing locations and areas. Each data format has its advantages, and both will be encountered as part of GIS&nbsp;operations.</p>
<p>Raster and vector data are commonly combined (e.g., extracting image information for a given location or clipping an image to an area of interest); however, there are also situations in which conversion between the two formats is useful. In making such conversions, it is important to consider the key advantages of each format. Rasters can store data efficiently where each pixel has a numerical value, while vector data can more effectively represent geometric features where homogenous areas have shared properties. Each format lends itself to distinctive analytical operations, and combining them can be powerful.</p>
<p>In this exercise, we’ll use topographic elevation and forest change images in Colombia as well as a protected area feature collection to practice the conversion between raster and vector formats, and to identify situations in which this is worthwhile.</p>
</section>
<section id="raster-to-vector-conversion" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="raster-to-vector-conversion"><span class="header-section-number">8.1</span> Raster to Vector Conversion&nbsp;</h2>
<section id="raster-to-polygons" class="level3" data-number="8.1.1">
<h3 data-number="8.1.1" class="anchored" data-anchor-id="raster-to-polygons"><span class="header-section-number">8.1.1</span> Raster to Polygons</h3>
<p>In this section we will convert an elevation image (raster) to a feature collection (vector). We will start by loading the&nbsp;Global Multi-Resolution Terrain Elevation Data 2010&nbsp;and the Global Administrative Unit Layers 2015&nbsp;dataset to focus on Colombia. The elevation image is a raster at 7.5 arc-second spatial resolution containing a continuous measure of elevation in meters in each pixel.</p>
<p>// Load raster (elevation) and vector (colombia) datasets.<br>
var&nbsp;elevation = ee.Image(‘USGS/GMTED2010’).rename(‘elevation’);<br>
var&nbsp;colombia = ee.FeatureCollection(&nbsp; &nbsp; &nbsp; &nbsp;‘FAO/GAUL_SIMPLIFIED_500m/2015/level0’)<br>
&nbsp; &nbsp;.filter(ee.Filter.equals(‘ADM0_NAME’, ‘Colombia’));</p>
<p>// Display elevation image.<br>
Map.centerObject(colombia, 7);<br>
Map.addLayer(elevation, {<br>
&nbsp; &nbsp;min: 0,<br>
&nbsp; &nbsp;max: 4000}, ‘Elevation’);</p>
<p>When converting an image to a feature collection, we will aggregate the categorical elevation values into a set of categories to create polygon shapes of connected pixels with similar elevations. For this exercise, we will create four zones of elevation by grouping the altitudes to 0-100 m = 0, 100–200 m = 1, 200–500 m = 2, and &gt;500 m = 3.</p>
<p>// Initialize image with zeros and define elevation zones.<br>
var&nbsp;zones = ee.Image(0)<br>
&nbsp; &nbsp;.where(elevation.gt(100), 1)<br>
&nbsp; &nbsp;.where(elevation.gt(200), 2)<br>
&nbsp; &nbsp;.where(elevation.gt(500), 3);</p>
<p>// Mask pixels below sea level (&lt;= 0 m) to retain only land areas.<br>
// Name the band with values 0-3 as ‘zone’.<br>
zones = zones.updateMask(elevation.gt(0)).rename(‘zone’);</p>
<p>Map.addLayer(zones, {<br>
&nbsp; &nbsp;min: 0,<br>
&nbsp; &nbsp;max: 3,<br>
&nbsp; &nbsp;palette: [‘white’, ‘yellow’, ‘lime’, ‘green’],<br>
&nbsp; &nbsp;opacity: 0.7}, ‘Elevation zones’);</p>
<p>We will convert this zonal elevation image in Colombia to polygon shapes, which is a vector format (termed a FeatureCollection&nbsp;in Earth Engine), using the ee.Image.reduceToVectors&nbsp;method. This will create polygons delineating connected pixels with the same value. In doing so, we will use the same projection and spatial resolution as the image. Please note that loading the vectorized image in the native resolution (231.92 m) takes time to execute. For faster visualization, we set a coarse scale of 1,000 m.</p>
<p>var&nbsp;projection = elevation.projection();<br>
var&nbsp;scale = elevation.projection().nominalScale();</p>
<p>var&nbsp;elevationVector = zones.reduceToVectors({<br>
&nbsp; &nbsp;geometry: colombia.geometry(),<br>
&nbsp; &nbsp;crs: projection,<br>
&nbsp; &nbsp;scale: 1000, // scale&nbsp; &nbsp;geometryType: ‘polygon’,<br>
&nbsp; &nbsp;eightConnected: false,<br>
&nbsp; &nbsp;labelProperty: ‘zone’,<br>
&nbsp; &nbsp;bestEffort: true,<br>
&nbsp; &nbsp;maxPixels: 1e13,<br>
&nbsp; &nbsp;tileScale: 3&nbsp;// In case of error.<br>
});</p>
<p>print(elevationVector.limit(10));</p>
<p>var&nbsp;elevationDrawn = elevationVector.draw({<br>
&nbsp; &nbsp;color: ‘black’,<br>
&nbsp; &nbsp;strokeWidth: 1<br>
});<br>
Map.addLayer(elevationDrawn, {}, ‘Elevation zone polygon’);</p>
<p><img src="F5/image50.png" class="img-fluid"></p>
<p><img src="F5/image33.png" class="img-fluid"></p>
<p><img src="F5/image36.png" class="img-fluid"></p>
<p><img src="F5/image7.png" class="img-fluid"></p>
<p>Fig. F5.1.1&nbsp;Raster-based elevation (top left) and zones (top right), vectorized elevation zones overlaid on the raster (bottom-left) and vectorized elevation zones only (bottom-right)</p>
<p>You may have realized that polygons consist of complex lines, including some small polygons with just one pixel. That happens when there are no surrounding pixels of the same elevation zone. You may not need a vector map with such details—if, for instance, you want to produce a regional or global map. We can use a&nbsp;morphological reducer&nbsp;focalMode&nbsp;to simplify the shape by defining a neighborhood size around a pixel. In this example, we will set the kernel radius as four pixels. This operation makes the resulting polygons look much smoother, but less precise (Fig. F5.1.2).</p>
<p>var&nbsp;zonesSmooth = zones.focalMode(4, ‘square’);</p>
<p>zonesSmooth = zonesSmooth.reproject(projection.atScale(scale));</p>
<p>Map.addLayer(zonesSmooth, {<br>
&nbsp; &nbsp;min: 1,<br>
&nbsp; &nbsp;max: 3,<br>
&nbsp; &nbsp;palette: [‘yellow’, ‘lime’, ‘green’],<br>
&nbsp; &nbsp;opacity: 0.7}, ‘Elevation zones (smooth)’);</p>
<p>var&nbsp;elevationVectorSmooth = zonesSmooth.reduceToVectors({<br>
&nbsp; &nbsp;geometry: colombia.geometry(),<br>
&nbsp; &nbsp;crs: projection,<br>
&nbsp; &nbsp;scale: scale,<br>
&nbsp; &nbsp;geometryType: ‘polygon’,<br>
&nbsp; &nbsp;eightConnected: false,<br>
&nbsp; &nbsp;labelProperty: ‘zone’,<br>
&nbsp; &nbsp;bestEffort: true,<br>
&nbsp; &nbsp;maxPixels: 1e13,<br>
&nbsp; &nbsp;tileScale: 3<br>
});</p>
<p>var&nbsp;smoothDrawn = elevationVectorSmooth.draw({<br>
&nbsp; &nbsp;color: ‘black’,<br>
&nbsp; &nbsp;strokeWidth: 1<br>
});<br>
Map.addLayer(smoothDrawn, {}, ‘Elevation zone polygon (smooth)’);</p>
<p>We can see now that the polygons have more distinct shapes with many fewer small polygons in the new map (Fig. F5.1.2). It is important to note that when you use methods like focalMode&nbsp;(or other, similar methods such as connectedComponents&nbsp;and connectedPixelCount), you need to reproject according to the original image in order to display properly with zoom using the interactive Code Editor.</p>
<p><img src="F5/image20.png" class="img-fluid"></p>
<p><img src="F5/image37.png" class="img-fluid"></p>
<p>Fig. F5.1.2&nbsp;Before (left) and after (right) applying focalMode</p>
</section>
<section id="raster-to-points" class="level3" data-number="8.1.2">
<h3 data-number="8.1.2" class="anchored" data-anchor-id="raster-to-points"><span class="header-section-number">8.1.2</span> Raster to Points</h3>
<p>Lastly, we will convert a small part of this elevation image into a point vector dataset. For this exercise, we will use the same example and build on the code from the previous subsection. This might be useful when you want to use geospatial data in a tabular format in combination with other conventional datasets such as economic indicators (Fig. F5.1.3).</p>
<p><img src="F5/image24.png" class="img-fluid"></p>
<p><img src="F5/image11.png" class="img-fluid"></p>
<p>Fig. F5.1.3&nbsp;Elevation point values with latitude and longitude</p>
<p>The easiest way to do this is to use sample&nbsp;while activating the geometries parameter. This will extract the points at the centroid of the elevation pixel.</p>
<p>var&nbsp;geometry = ee.Geometry.Polygon([<br>
&nbsp; &nbsp;[-89.553, -0.929],<br>
&nbsp; &nbsp;[-89.436, -0.929],<br>
&nbsp; &nbsp;[-89.436, -0.866],<br>
&nbsp; &nbsp;[-89.553, -0.866],<br>
&nbsp; &nbsp;[-89.553, -0.929]<br>
]);</p>
<p>// To zoom into the area, un-comment and run below<br>
// Map.centerObject(geometry,12);<br>
Map.addLayer(geometry, {}, ‘Areas to extract points’);</p>
<p>var&nbsp;elevationSamples = elevation.sample({<br>
&nbsp; &nbsp;region: geometry,<br>
&nbsp; &nbsp;projection: projection,<br>
&nbsp; &nbsp;scale: scale,<br>
&nbsp; &nbsp;geometries: true,<br>
});</p>
<p>Map.addLayer(elevationSamples, {}, ‘Points extracted’);</p>
<p>// Add three properties to the output table:<br>
// ‘Elevation’, ‘Longitude’, and ‘Latitude’.<br>
elevationSamples = elevationSamples.map(function(feature) {&nbsp; &nbsp;var&nbsp;geom = feature.geometry().coordinates();&nbsp; &nbsp;return&nbsp;ee.Feature(null, {&nbsp; &nbsp; &nbsp; &nbsp;‘Elevation’: ee.Number(feature.get(&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘elevation’)),&nbsp; &nbsp; &nbsp; &nbsp;‘Long’: ee.Number(geom.get(0)),&nbsp; &nbsp; &nbsp; &nbsp;‘Lat’: ee.Number(geom.get(1))<br>
&nbsp; &nbsp;});<br>
});</p>
<p>// Export as CSV.<br>
Export.table.toDrive({<br>
&nbsp; &nbsp;collection: elevationSamples,<br>
&nbsp; &nbsp;description: ‘extracted_points’,<br>
&nbsp; &nbsp;fileFormat: ‘CSV’<br>
});</p>
<p>We can also extract sample points per elevation zone. Below is an example of extracting 10 randomly selected points per elevation zone (Fig. F5.1.4). You can also set different values for each zone using classValues&nbsp;and classPoints&nbsp;parameters to modify the sampling intensity in each class. This may be useful, for instance, to generate point samples for a validation effort.</p>
<p>var&nbsp;elevationSamplesStratified = zones.stratifiedSample({<br>
&nbsp; &nbsp;numPoints: 10,<br>
&nbsp; &nbsp;classBand: ‘zone’,<br>
&nbsp; &nbsp;region: geometry,<br>
&nbsp; &nbsp;scale: scale,<br>
&nbsp; &nbsp;projection: projection,<br>
&nbsp; &nbsp;geometries: true<br>
});</p>
<p>Map.addLayer(elevationSamplesStratified, {}, ‘Stratified samples’);</p>
<p><img src="F5/image23.png" class="img-fluid"></p>
<p>Fig. F5.1.4&nbsp;Stratified sampling over different elevation zones</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F51a.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.</p>
</div>
</div>
<p>##3. A More Complex Example</p>
<p>In this section we’ll use two global datasets, one to represent raster formats and the other vectors:</p>
<ul>
<li>The Global Forest Change&nbsp;(GFC) dataset: a raster dataset describing global tree cover and change for 2001–present.</li>
<li>The World Protected Areas Database: a vector database of global protected areas.</li>
</ul>
<p>The objective will be to combine these two datasets to quantify rates of deforestation in protected areas in the “arc of deforestation” of the Colombian Amazon. The datasets can be loaded into Earth Engine with the following code:</p>
<p>// Read input data.<br>
// Note: these datasets are periodically updated.<br>
// Consider searching the Data Catalog for newer versions.<br>
var&nbsp;gfc = ee.Image(‘UMD/hansen/global_forest_change_2020_v1_8’);<br>
var&nbsp;wdpa = ee.FeatureCollection(‘WCMC/WDPA/current/polygons’);</p>
<p>// Print assets to show available layers and properties.<br>
print(gfc);<br>
print(wdpa.limit(10)); // Show first 10 records.</p>
<p>The GFC dataset&nbsp;(first presented in detail in Chap. F1.1) is a global set of rasters that quantify tree cover and change for the period beginning in 2001. We’ll use a single image from this dataset:</p>
<ul>
<li>‘lossyear’: a categorical raster of forest loss (1–20, corresponding to deforestation for the period 2001–2020), and 0 for no change</li>
</ul>
<p>The World Database on Protected Areas (WDPA) is a harmonized dataset of global terrestrial and marine protected area locations, along with details on the classification and management of each. In addition to protected area outlines, we’ll use two fields from this database:</p>
<ul>
<li>‘NAME’’: the name of each protected area</li>
<li>‘WDPA_PID’: a unique numerical ID for each protected area</li>
</ul>
<p>To begin with, we’ll focus on forest change dynamics in ‘La Paya’, a small protected area in the Colombian Amazon. We’ll first visualize these data&nbsp;using the paint&nbsp;command, which is discussed in more detail in Chap. F5.3:</p>
<p>// Display deforestation.<br>
var&nbsp;deforestation = gfc.select(‘lossyear’);</p>
<p>Map.addLayer(deforestation, {<br>
&nbsp; &nbsp;min: 1,<br>
&nbsp; &nbsp;max: 20,<br>
&nbsp; &nbsp;palette: [‘yellow’, ‘orange’, ‘red’]<br>
}, ‘Deforestation raster’);</p>
<p>// Display WDPA data.<br>
var&nbsp;protectedArea = wdpa.filter(ee.Filter.equals(‘NAME’, ‘La Paya’));</p>
<p>// Display protected area as an outline (see F5.3 for paint()).<br>
var&nbsp;protectedAreaOutline = ee.Image().byte().paint({<br>
&nbsp; &nbsp;featureCollection: protectedArea,<br>
&nbsp; &nbsp;color: 1,<br>
&nbsp; &nbsp;width: 3<br>
});</p>
<p>Map.addLayer(protectedAreaOutline, {<br>
&nbsp; &nbsp;palette: ‘white’}, ‘Protected area’);</p>
<p>// Set up map display.<br>
Map.centerObject(protectedArea);<br>
Map.setOptions(‘SATELLITE’);</p>
<p>This will display the boundary of the La Paya protected area and deforestation in the region (Fig. F5.1.5).</p>
<p><img src="F5/image55.png" class="img-fluid"></p>
<p>Fig. F5.1.5&nbsp;View of the La Paya protected area in the Colombian Amazon (in white), and deforestation over the period 2001–2020 (in yellows and reds, with darker colors indicating more recent changes)</p>
<p>We can use Earth Engine to convert the deforestation raster to a set of polygons. The deforestation data are appropriate for this transformation as each deforestation event is labeled categorically by year, and change events are spatially contiguous. This is performed in Earth Engine using the ee.Image.reduceToVectors&nbsp;method, as described earlier in this section. &nbsp;</p>
<p>// Convert from a deforestation raster to vector.<br>
var&nbsp;deforestationVector = deforestation.reduceToVectors({<br>
&nbsp; &nbsp;scale: deforestation.projection().nominalScale(),<br>
&nbsp; &nbsp;geometry: protectedArea.geometry(),<br>
&nbsp; &nbsp;labelProperty: ‘lossyear’, // Label polygons with a change year.&nbsp; &nbsp;maxPixels: 1e13<br>
});</p>
<p>// Count the number of individual change events<br>
print(‘Number of change events:’, deforestationVector.size());</p>
<p>// Display deforestation polygons. Color outline by change year.<br>
var&nbsp;deforestationVectorOutline = ee.Image().byte().paint({<br>
&nbsp; &nbsp;featureCollection: deforestationVector,<br>
&nbsp; &nbsp;color: ‘lossyear’,<br>
&nbsp; &nbsp;width: 1<br>
});</p>
<p>Map.addLayer(deforestationVectorOutline, {<br>
&nbsp; &nbsp;palette: [‘yellow’, ‘orange’, ‘red’],<br>
&nbsp; &nbsp;min: 1,<br>
&nbsp; &nbsp;max: 20}, ‘Deforestation vector’);</p>
<p>Fig. F5.1.6 shows a comparison of the raster versus vector representations of deforestation within the protected area.</p>
<p><img src="F5/image42.png" class="img-fluid"></p>
<p><img src="F5/image13.png" class="img-fluid"></p>
<p>Fig. F5.1.6&nbsp;Raster (left) versus vector (right) representations of deforestation data of the La Paya protected area</p>
<p>Having converted from raster to vector, a new set of operations becomes available for post-processing the deforestation data. We might, for instance, be interested in the number of individual change events each year (Fig. F5.1.7):</p>
<p>var&nbsp;chart = ui.Chart.feature<br>
&nbsp; &nbsp;.histogram({<br>
&nbsp; &nbsp; &nbsp; &nbsp;features: deforestationVector,<br>
&nbsp; &nbsp; &nbsp; &nbsp;property: ‘lossyear’&nbsp; &nbsp;})<br>
&nbsp; &nbsp;.setOptions({<br>
&nbsp; &nbsp; &nbsp; &nbsp;hAxis: {<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;title: ‘Year’&nbsp; &nbsp; &nbsp; &nbsp;},<br>
&nbsp; &nbsp; &nbsp; &nbsp;vAxis: {<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;title: ‘Number of deforestation events’&nbsp; &nbsp; &nbsp; &nbsp;},<br>
&nbsp; &nbsp; &nbsp; &nbsp;legend: {<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;position: ‘none’&nbsp; &nbsp; &nbsp; &nbsp;}<br>
&nbsp; &nbsp;});print(chart);</p>
<p><img src="F5/image15.png" class="img-fluid"></p>
<p>Fig. F5.1.7&nbsp;Plot of the number of deforestation events in La Paya for the years 2001–2020</p>
<p>There might also be interest in generating point locations for individual change events (e.g., to aid a field campaign):</p>
<p>// Generate deforestation point locations.<br>
var&nbsp;deforestationCentroids = deforestationVector.map(function(feat) {&nbsp; &nbsp;return&nbsp;feat.centroid();<br>
});</p>
<p>Map.addLayer(deforestationCentroids, {<br>
&nbsp; &nbsp;color: ‘darkblue’}, ‘Deforestation centroids’);</p>
<p>The vector format allows for easy filtering to only deforestation events of interest, such as only the largest deforestation events:</p>
<p>// Add a new property to the deforestation FeatureCollection<br>
// describing the area of the change polygon.<br>
deforestationVector = deforestationVector.map(function(feat) {&nbsp; &nbsp;return&nbsp;feat.set(‘area’, feat.geometry().area({<br>
&nbsp; &nbsp; &nbsp; &nbsp;maxError: 10&nbsp; &nbsp;}).divide(10000)); // Convert m^2 to hectare.<br>
});</p>
<p>// Filter the deforestation FeatureCollection for only large-scale (&gt;10 ha) changes<br>
var&nbsp;deforestationLarge = deforestationVector.filter(ee.Filter.gt(&nbsp; &nbsp;‘area’, 10));</p>
<p>// Display deforestation area outline by year.<br>
var&nbsp;deforestationLargeOutline = ee.Image().byte().paint({<br>
&nbsp; &nbsp;featureCollection: deforestationLarge,<br>
&nbsp; &nbsp;color: ‘lossyear’,<br>
&nbsp; &nbsp;width: 1<br>
});</p>
<p>Map.addLayer(deforestationLargeOutline, {<br>
&nbsp; &nbsp;palette: [‘yellow’, ‘orange’, ‘red’],<br>
&nbsp; &nbsp;min: 1,<br>
&nbsp; &nbsp;max: 20}, ‘Deforestation (&gt;10 ha)’);</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F51b.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.</p>
</div>
</div>
</section>
<section id="raster-properties-to-vector-fields" class="level3" data-number="8.1.3">
<h3 data-number="8.1.3" class="anchored" data-anchor-id="raster-properties-to-vector-fields"><span class="header-section-number">8.1.3</span> Raster Properties to Vector Fields</h3>
<p>Sometimes we want to extract information from a raster to be included in an existing vector dataset. An example might be estimating a deforestation rate for a set of protected areas. Rather than perform this task on a case-by-case basis, we can attach information generated from an image as a property of a feature.</p>
<p>The following script shows how this can be used to quantify a deforestation rate for a set of protected areas in the Colombian Amazon.</p>
<p>// Load required datasets.<br>
var&nbsp;gfc = ee.Image(‘UMD/hansen/global_forest_change_2020_v1_8’);<br>
var&nbsp;wdpa = ee.FeatureCollection(‘WCMC/WDPA/current/polygons’);</p>
<p>// Display deforestation.<br>
var&nbsp;deforestation = gfc.select(‘lossyear’);</p>
<p>Map.addLayer(deforestation, {<br>
&nbsp; &nbsp;min: 1,<br>
&nbsp; &nbsp;max: 20,<br>
&nbsp; &nbsp;palette: [‘yellow’, ‘orange’, ‘red’]<br>
}, ‘Deforestation raster’);</p>
<p>// Select protected areas in the Colombian Amazon.<br>
var&nbsp;amazonianProtectedAreas = [&nbsp; &nbsp;‘Cordillera de los Picachos’, ‘La Paya’, ‘Nukak’,&nbsp; &nbsp;‘Serrania de Chiribiquete’,&nbsp; &nbsp;‘Sierra de la Macarena’, ‘Tinigua’<br>
];</p>
<p>var&nbsp;wdpaSubset = wdpa.filter(ee.Filter.inList(‘NAME’,<br>
&nbsp; &nbsp;amazonianProtectedAreas));</p>
<p>// Display protected areas as an outline.<br>
var&nbsp;protectedAreasOutline = ee.Image().byte().paint({<br>
&nbsp; &nbsp;featureCollection: wdpaSubset,<br>
&nbsp; &nbsp;color: 1,<br>
&nbsp; &nbsp;width: 1<br>
});</p>
<p>Map.addLayer(protectedAreasOutline, {<br>
&nbsp; &nbsp;palette: ‘white’}, ‘Amazonian protected areas’);</p>
<p>// Set up map display.<br>
Map.centerObject(wdpaSubset);<br>
Map.setOptions(‘SATELLITE’);</p>
<p>var&nbsp;scale = deforestation.projection().nominalScale();</p>
<p>// Use ‘reduceRegions’ to sum together pixel areas in each protected area.<br>
wdpaSubset = deforestation.gte(1)<br>
&nbsp; &nbsp;.multiply(ee.Image.pixelArea().divide(10000)).reduceRegions({<br>
&nbsp; &nbsp; &nbsp; &nbsp;collection: wdpaSubset,<br>
&nbsp; &nbsp; &nbsp; &nbsp;reducer: ee.Reducer.sum().setOutputs([&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘deforestation_area’]),<br>
&nbsp; &nbsp; &nbsp; &nbsp;scale: scale<br>
&nbsp; &nbsp;});</p>
<p>print(wdpaSubset); // Note the new ‘deforestation_area’ property.</p>
<p>The output of this script is an estimate of deforested area in hectares for each reserve. However, as reserve sizes vary substantially by area, we can normalize by the total area of each reserve to quantify rates of change.</p>
<p>// Normalize by area.<br>
wdpaSubset = wdpaSubset.map(&nbsp; &nbsp;function(feat) {&nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;feat.set(‘deforestation_rate’,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ee.Number(feat.get(‘deforestation_area’))<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.divide(feat.area().divide(10000)) // m2 to ha&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.divide(20) // number of years&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.multiply(100)); // to percentage points&nbsp; &nbsp;});// Print to identify rates of change per protected area.<br>
// Which has the fastest rate of loss?<br>
print(wdpaSubset.reduceColumns({<br>
&nbsp; &nbsp;reducer: ee.Reducer.toList().repeat(2),<br>
&nbsp; &nbsp;selectors: [‘NAME’, ‘deforestation_rate’]<br>
}));</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F51c.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.</p>
</div>
</div>
</section>
</section>
<section id="vector-to-raster-conversion" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="vector-to-raster-conversion"><span class="header-section-number">8.2</span> Vector-to-Raster Conversion</h2>
<p>In&nbsp;Sect. 1, we used the protected area feature collection as its original vector format. In this section, we will rasterize the protected area polygons to produce a mask and use this to assess rates of forest change.</p>
<section id="polygons-to-a-mask" class="level3" data-number="8.2.1">
<h3 data-number="8.2.1" class="anchored" data-anchor-id="polygons-to-a-mask"><span class="header-section-number">8.2.1</span> Polygons to a Mask</h3>
<p>The most common operation to convert from vector to raster is the production of binary image masks, describing whether a pixel intersects a line or falls within a polygon. To convert from vector to a raster mask, we can use the ee.FeatureCollection.reduceToImage&nbsp;method. Let’s continue with our example of the WDPA database and Global Forest Change data from the previous section:</p>
<p>// Load required datasets.<br>
var&nbsp;gfc = ee.Image(‘UMD/hansen/global_forest_change_2020_v1_8’);<br>
var&nbsp;wdpa = ee.FeatureCollection(‘WCMC/WDPA/current/polygons’);</p>
<p>// Get deforestation.<br>
var&nbsp;deforestation = gfc.select(‘lossyear’);</p>
<p>// Generate a new property called ‘protected’ to apply to the output mask.<br>
var&nbsp;wdpa = wdpa.map(function(feat) {&nbsp; &nbsp;return&nbsp;feat.set(‘protected’, 1);<br>
});</p>
<p>// Rasterize using the new property.<br>
// unmask() sets areas outside protected area polygons to 0.<br>
var&nbsp;wdpaMask = wdpa.reduceToImage([‘protected’], ee.Reducer.first())<br>
&nbsp; &nbsp;.unmask();</p>
<p>// Center on Colombia.<br>
Map.setCenter(-75, 3, 6);</p>
<p>// Display on map.<br>
Map.addLayer(wdpaMask, {<br>
&nbsp; &nbsp;min: 0,<br>
&nbsp; &nbsp;max: 1}, ‘Protected areas (mask)’);</p>
<p>We can use this mask to, for example, highlight only deforestation that occurs within a protected area using logical operations:</p>
<p>// Set the deforestation layer to 0 where outside a protected area.<br>
var&nbsp;deforestationProtected = deforestation.where(wdpaMask.eq(0), 0);</p>
<p>// Update mask to hide where deforestation layer = 0<br>
var&nbsp;deforestationProtected = deforestationProtected<br>
&nbsp; &nbsp;.updateMask(deforestationProtected.gt(0));</p>
<p>// Display deforestation in protected areas<br>
Map.addLayer(deforestationProtected, {<br>
&nbsp; &nbsp;min: 1,<br>
&nbsp; &nbsp;max: 20,<br>
&nbsp; &nbsp;palette: [‘yellow’, ‘orange’, ‘red’]<br>
}, ‘Deforestation protected’);</p>
<p>In the above example we generated a simple binary mask, but reduceToImage&nbsp;can also preserve a numerical property of the input polygons. For example, we might want to be able to determine which protected area each pixel represents. In this case, we can produce an image with the unique ID of each protected area:</p>
<p>// Produce an image with unique ID of protected areas.<br>
var&nbsp;wdpaId = wdpa.reduceToImage([‘WDPAID’], ee.Reducer.first());</p>
<p>Map.addLayer(wdpaId, {<br>
&nbsp; &nbsp;min: 1,<br>
&nbsp; &nbsp;max: 100000}, ‘Protected area ID’);</p>
<p>This output can be useful when performing large-scale raster operations, such as efficiently calculating deforestation rates for multiple protected areas.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F51d.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.</p>
</div>
</div>
</section>
<section id="a-more-complex-example" class="level3" data-number="8.2.2">
<h3 data-number="8.2.2" class="anchored" data-anchor-id="a-more-complex-example"><span class="header-section-number">8.2.2</span> A More Complex Example</h3>
<p>The reduceToImage&nbsp;method is not the only way to convert a feature collection to an image. We will create a distance image layer from the boundary of the protected area using distance. For this example, we return to the La Paya protected area explored in Sect. 1.</p>
<p>// Load required datasets.<br>
var&nbsp;gfc = ee.Image(‘UMD/hansen/global_forest_change_2020_v1_8’);<br>
var&nbsp;wdpa = ee.FeatureCollection(‘WCMC/WDPA/current/polygons’);</p>
<p>// Select a single protected area.<br>
var&nbsp;protectedArea = wdpa.filter(ee.Filter.equals(‘NAME’, ‘La Paya’));</p>
<p>// Maximum distance in meters is set in the brackets.<br>
var&nbsp;distance = protectedArea.distance(1000000);</p>
<p>Map.addLayer(distance, {<br>
&nbsp; &nbsp;min: 0,<br>
&nbsp; &nbsp;max: 20000,<br>
&nbsp; &nbsp;palette: [‘white’, ‘grey’, ‘black’],<br>
&nbsp; &nbsp;opacity: 0.6}, ‘Distance’);</p>
<p>Map.centerObject(protectedArea);</p>
<p>We can also show the distance inside and outside of the boundary by using the rasterized protected area (Fig. F5.1.8).</p>
<p>// Produce a raster of inside/outside the protected area.<br>
var&nbsp;protectedAreaRaster = protectedArea.map(function(feat) {&nbsp; &nbsp;return&nbsp;feat.set(‘protected’, 1);<br>
}).reduceToImage([‘protected’], ee.Reducer.first());</p>
<p>Map.addLayer(distance.updateMask(protectedAreaRaster), {<br>
&nbsp; &nbsp;min: 0,<br>
&nbsp; &nbsp;max: 20000}, ‘Distance inside protected area’);</p>
<p>Map.addLayer(distance.updateMask(protectedAreaRaster.unmask()<br>
.not()), {<br>
&nbsp; &nbsp;min: 0,<br>
&nbsp; &nbsp;max: 20000}, ‘Distance outside protected area’);</p>
<p><img src="F5/image56.png" class="img-fluid"></p>
<p><img src="F5/image9.png" class="img-fluid"></p>
<p><img src="F5/image25.png" class="img-fluid"></p>
<p>Fig. F5.1.8&nbsp;Distance from the La Paya boundary (left), distance within the La Paya (middle), and distance outside the La Paya (right)</p>
<p>Sometimes it makes sense to work with objects in raster imagery. This is an unusual case of vector-like operations conducted with raster data. There is a good reason for this where the vector equivalent would be computationally burdensome.</p>
<p>An example of this is estimating deforestation rates by distance to the edge of the protected area, as it is common that rates of change will be higher at the boundary of a protected area. We will create a distance raster with three zones from the La Paya boundary (&gt;1 km, &gt;2 km, &gt;3 km, and &gt;4 km) and to estimate the deforestation by distance from the boundary (Fig. F5.1.9).</p>
<p>var&nbsp;distanceZones = ee.Image(0)<br>
&nbsp; &nbsp;.where(distance.gt(0), 1)<br>
&nbsp; &nbsp;.where(distance.gt(1000), 2)<br>
&nbsp; &nbsp;.where(distance.gt(3000), 3)<br>
&nbsp; &nbsp;.updateMask(distance.lte(5000));</p>
<p>Map.addLayer(distanceZones, {}, ‘Distance zones’);</p>
<p>var&nbsp;deforestation = gfc.select(‘loss’);<br>
var&nbsp;deforestation1km = deforestation.updateMask(distanceZones.eq(1));<br>
var&nbsp;deforestation3km = deforestation.updateMask(distanceZones.lte(2));<br>
var&nbsp;deforestation5km = deforestation.updateMask(distanceZones.lte(3));</p>
<p>Map.addLayer(deforestation1km, {<br>
&nbsp; &nbsp;min: 0,<br>
&nbsp; &nbsp;max: 1}, ‘Deforestation within a 1km buffer’);<br>
Map.addLayer(deforestation3km, {<br>
&nbsp; &nbsp;min: 0,<br>
&nbsp; &nbsp;max: 1,<br>
&nbsp; &nbsp;opacity: 0.5}, ‘Deforestation within a 3km buffer’);<br>
Map.addLayer(deforestation5km, {<br>
&nbsp; &nbsp;min: 0,<br>
&nbsp; &nbsp;max: 1,<br>
&nbsp; &nbsp;opacity: 0.5}, ‘Deforestation within a 5km buffer’);</p>
<p><img src="F5/image22.png" class="img-fluid"></p>
<p><img src="F5/image6.png" class="img-fluid"></p>
<p><img src="F5/image21.png" class="img-fluid"></p>
<p><img src="F5/image26.png" class="img-fluid"></p>
<p>Fig. F5.1.9&nbsp;Distance zones (top left) and deforestation by zone (&lt;1 km, &lt;3 km, and &lt;5 km)</p>
<p>Lastly, we can estimate the deforestation area within 1 km of the protected area but only outside of the boundary.</p>
<p>var&nbsp;deforestation1kmOutside = deforestation1km<br>
&nbsp; &nbsp;.updateMask(protectedAreaRaster.unmask().not());</p>
<p>// Get the value of each pixel in square meters<br>
// and divide by 10000 to convert to hectares.<br>
var&nbsp;deforestation1kmOutsideArea = deforestation1kmOutside.eq(1)<br>
&nbsp; &nbsp;.multiply(ee.Image.pixelArea()).divide(10000);</p>
<p>// We need to set a larger geometry than the protected area<br>
// for the geometry parameter in reduceRegion().<br>
var&nbsp;deforestationEstimate = deforestation1kmOutsideArea<br>
&nbsp; &nbsp;.reduceRegion({<br>
&nbsp; &nbsp; &nbsp; &nbsp;reducer: ee.Reducer.sum(),<br>
&nbsp; &nbsp; &nbsp; &nbsp;geometry: protectedArea.geometry().buffer(1000),<br>
&nbsp; &nbsp; &nbsp; &nbsp;scale: deforestation.projection().nominalScale()<br>
&nbsp; &nbsp;});</p>
<p>print(‘Deforestation within a 1km buffer outside the protected area (ha)’,<br>
&nbsp; &nbsp;deforestationEstimate);</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F51e.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.</p>
</div>
</div>
</section>
</section>
<section id="synthesis-1" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="synthesis-1">Synthesis</h2>
<p>Question 1. In this lab, we quantified rates of deforestation in La Paya. There is another protected area in the Colombian Amazon named Tinigua. By modifying the existing scripts, determine how the dynamics of forest change in Tinigua compare to those in La Paya&nbsp;with respect to:</p>
<ul>
<li>the number of deforestation events</li>
<li>the year with the greatest number of change events</li>
<li>the mean average area of change events</li>
<li>the total area of loss</li>
</ul>
<p>Question 2. In Sect. 1.4, we only considered losses of tree cover, but many protected areas will also have increases in tree cover from regrowth (which is typical of shifting agriculture). Calculate growth in hectares using the Global Forest Change dataset’s gain layer for the six protected areas in Sect. 1.4 by extracting the raster properties and adding them to vector fields. Which has the greatest area of regrowth? Is this likely to be sufficient to balance out the rates of forest loss? Note: The gain layer shows locations where tree cover has increased for the period 2001–2012 (0 = no gain, 1 = tree cover increase), so for comparability use deforestation between the same time period of 2001–2012.</p>
<p>Question 3. In Sect. 2.2, we considered rates of deforestation in a buffer zone around La Paya. Estimate the deforestation rates inside of La Paya using buffer zones. Is forest loss more common close to the boundary of the reserve?</p>
<p>Question 4. Sometimes it’s advantageous to perform processing using raster operations, particularly at large scales. It is possible to perform many of the tasks in Sect. 1.3 and 1.4 by first converting the protected area vector to raster, and then using only raster operations. As an example, can you display only deforestation events &gt;10 ha in La Paya using only raster data? (Hint: Consider using&nbsp;ee.Image.connectedPixelCount. You may also want to also look at Sect. 2.1).</p>
</section>
<section id="conclusion-1" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="conclusion-1">Conclusion</h2>
<p>In this chapter, you learned how to convert raster to vector and vice versa. More importantly, you now have a better understanding of why and when such conversions are useful. Our examples should give you practical applications and ideas for using these techniques.</p>
</section>
</section>
<section id="zonal-statistics" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Zonal Statistics</h1>
<p>::: {.callout-tip} # Chapter Information</p>
<section id="author-2" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="author-2">Author</h2>
<p>&nbsp;</p>
<p>Sara Winsemius and Justin Braaten</p>
</section>
<section id="overview-1" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="overview-1">Overview</h2>
<p>The purpose of this chapter is to extract values from rasters for intersecting points or polygons. We will lay out the process and a function to calculate zonal statistics, which includes optional parameters to modify the function, and then apply the process to three examples using different raster datasets and combinations of parameters.</p>
</section>
<section id="learning-outcomes-1" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="learning-outcomes-1">Learning Outcomes</h2>
<p>&nbsp;</p>
<ul>
<li>Buffering points as square or circular regions.</li>
<li>Writing and applying functions with optional parameters.</li>
<li>Learning what zonal statistics are and how to use reducers.</li>
<li>Exporting computation results to a table.</li>
<li>Copying properties from one image to another.</li>
</ul>
</section>
<section id="assumes-you-know-how-to-1" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="assumes-you-know-how-to-1">Assumes you know how to:</h2>
<ul>
<li>Recognize similarities and differences among Landsat 5, 7, and 8 spectral bands (Part F1, Part F2, Part F3).</li>
<li>Understand distinctions among Image, ImageCollection, Feature&nbsp;and FeatureCollection&nbsp;Earth Engine objects&nbsp;(Part F1, Part F2, Part F5).</li>
<li>Use drawing tools to create points, lines, and polygons (Chap. F2.1).</li>
<li>Write a function and map&nbsp;it over an ImageCollection&nbsp;(Chap. F4.0).</li>
<li>Mask cloud, cloud shadow, snow/ice, and other undesired pixels (Chap. F4.3).</li>
<li>Export calculated data to tables with Tasks (Chap. F5.0).</li>
<li>Understand the differences between raster and vector data&nbsp;(Chap. F5.0, Chap. F5.1).</li>
<li>Write a function and map&nbsp;it over a FeatureCollection&nbsp;(Chap. F5.1).</li>
</ul>
</section>
<section id="introduction-to-theory" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="introduction-to-theory"><span class="header-section-number">9.1</span> Introduction to Theory&nbsp;</h2>
<p>Anyone working with field data collected at plots will likely need to summarize raster-based data associated with those plots. For instance, they need to know the Normalized Difference Vegetation Index (NDVI), precipitation, or elevation for each plot (or surrounding region). Calculating statistics from a raster within given regions is called zonal statistics. Zonal statistics were calculated in Chaps. F5.0 and F5.1&nbsp;using ee.Image.ReduceRegions. Here, we present a more general approach to calculating zonal statistics with a custom function that works for both ee.Image&nbsp;and ee.ImageCollection&nbsp;objects. In addition to its flexibility, the reduction method used here is less prone to “Computed value is too large”&nbsp;errors that can occur when using ReduceRegions&nbsp;with very large or complex ee.FeatureCollection&nbsp;object inputs.</p>
<p>The&nbsp;zonal statistics function in this chapter works for an Image&nbsp;or an ImageCollection. Running the function over an ImageCollection&nbsp;will produce a table with values from each image in the collection per point. Image collections can be processed before extraction as needed—for example, by masking clouds from satellite imagery or by constraining the dates needed for a particular research question. In this tutorial, the data extracted from rasters are exported to a table for analysis, where each row of the table corresponds to a unique point-image combination.</p>
<p>In fieldwork, researchers often work with plots, which are commonly recorded as polygon files or as a center point with a set radius. It is rare that plots will be set directly in the center of pixels from your desired raster dataset, and many field GPS units have positioning errors. Because of these issues, it may be important to use a statistic of adjacent pixels (as described in Chap. F3.2) to estimate the central value in what’s often called a neighborhood mean or focal mean (Cansler and McKenzie 2012, Miller and Thode 2007).</p>
<p>To choose the size of your neighborhood, you will need to consider your research questions, the spatial resolution of the dataset, the size of your field plot, and the error from your GPS. For example, the raster value extracted for randomly placed 20 m diameter plots would likely merit use of a neighborhood mean when using Sentinel-2 or Landsat 8—at 10 m and 30 m spatial resolution, respectively—while using a thermal band from MODIS (Moderate Resolution Imaging Spectroradiometer) at 1000 m may not. While much of this tutorial is written with plot points and buffers in mind, a polygon asset with predefined regions will serve the same purpose.</p>
</section>
<section id="functions" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="functions"><span class="header-section-number">9.2</span> Functions</h2>
<p>Two functions are provided; copy and paste them into your script:</p>
<ul>
<li>A function to generate circular or square regions from buffered points</li>
<li>A function to extract image pixel neighborhood statistics for a given region</li>
</ul>
<section id="function-bufferpointsradius-bounds" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="function-bufferpointsradius-bounds"><span class="header-section-number">9.2.1</span> Function: bufferPoints(radius, bounds)</h3>
<p>Our first function, bufferPoints, returns a function for adding a buffer to points and optionally transforming to rectangular bounds (see Table F5.2.1).</p>
<p>Table F5.2.1&nbsp;Parameters for bufferPoints</p>
<p>Parameter</p>
<p>Type</p>
<p>Description</p>
<p>radius</p>
<p>Number</p>
<p>buffer radius (m).</p>
<p>[bounds=false]</p>
<p>Boolean</p>
<p>An optional flag indicating whether to transform buffered point (i.e., a circle) to square bounds.</p>
<p>function&nbsp;bufferPoints(radius, bounds) {&nbsp; &nbsp;return&nbsp;function(pt) {<br>
&nbsp; &nbsp; &nbsp; &nbsp;pt = ee.Feature(pt);&nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;bounds ? pt.buffer(radius).bounds() : pt.buffer(<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;radius);<br>
&nbsp; &nbsp;};<br>
}</p>
</section>
<section id="function-zonalstatsfc-params" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="function-zonalstatsfc-params"><span class="header-section-number">9.2.2</span> Function: zonalStats(fc, params)</h3>
<p>The second function, zonalStats, reduces images in an ImageCollection&nbsp;by regions defined in a FeatureCollection. Note that reductions can return null statistics that you might want to filter out of the resulting feature collection. Null statistics occur when there are no valid pixels intersecting the region being reduced. This situation can be caused by points that are outside of an image or in regions that are masked for quality or clouds.</p>
<p>This function is written to include many optional parameters (see Table F5.2.2). Look at the function carefully and note how it is written to include defaults that make it easy to apply the basic function while allowing customization.</p>
<p>Table&nbsp;F5.2.2&nbsp;Parameters for zonalStats</p>
<p>Parameter</p>
<p>Type</p>
<p>Description</p>
<p>ic</p>
<p>ee.ImageCollection</p>
<p>Image collection from which to extract values.</p>
<p>fc</p>
<p>ee.FeatureCollection</p>
<p>Feature collection that provides regions/zones by which to reduce image pixels.</p>
<p>[params]</p>
<p>Object</p>
<p>An optional Object that provides function arguments.</p>
<p>[params.reducer=ee.Reducer.mean()]</p>
<p>ee.Reducer</p>
<p>The reducer to apply. Optional.</p>
<p>[params.scale=null]</p>
<p>Number</p>
<p>A nominal scale in meters of the projection to work in. If null, the native nominal image scale is used. Optional.</p>
<p>[params.crs=null]</p>
<p>String</p>
<p>The projection to work in. If null, the native image Coordinate Reference System (CRS) is used. Optional.</p>
<p>[params.bands=null]</p>
<p>Array</p>
<p>A list of image band names for which to reduce values. If null, all bands will be reduced. Band names define column names in the resulting reduction table. Optional.</p>
<p>[params.bandsRename=null]</p>
<p>Array</p>
<p>A list of desired image band names. The length and order must correspond to the params.bands list. If null, band names will be unchanged. Band names define column names in the resulting reduction table. Optional.</p>
<p>[params.imgProps=null]</p>
<p>Array</p>
<p>A list of image properties to include in the table of region reduction results. If null, all image properties are included. Optional.</p>
<p>[params.imgPropsRename=null]</p>
<p>Array</p>
<p>A list of image property names to replace those provided by params.imgProps. The length and order must match the params.imgProps entries. Optional.</p>
<p>[params.datetimeName=’datetime]</p>
<p>String</p>
<p>The desired name of the datetime field. The datetime refers to the ‘system:time_start’ value of the ee.Image being reduced. Optional.</p>
<p>[params.datetimeFormat=’YYYY-MM-dd HH:mm:ss]</p>
<p>String</p>
<p>The desired datetime format. Use ISO 8601 data string standards. The datetime string is derived from the ‘system:time_start’ value of the ee.Image being reduced.&nbsp;Optional.</p>
<p>function&nbsp;zonalStats(ic, fc, params) {&nbsp; &nbsp;// Initialize internal params dictionary.&nbsp; &nbsp;var&nbsp;_params = {<br>
&nbsp; &nbsp; &nbsp; &nbsp;reducer: ee.Reducer.mean(),<br>
&nbsp; &nbsp; &nbsp; &nbsp;scale: null,<br>
&nbsp; &nbsp; &nbsp; &nbsp;crs: null,<br>
&nbsp; &nbsp; &nbsp; &nbsp;bands: null,<br>
&nbsp; &nbsp; &nbsp; &nbsp;bandsRename: null,<br>
&nbsp; &nbsp; &nbsp; &nbsp;imgProps: null,<br>
&nbsp; &nbsp; &nbsp; &nbsp;imgPropsRename: null,<br>
&nbsp; &nbsp; &nbsp; &nbsp;datetimeName: ‘datetime’,<br>
&nbsp; &nbsp; &nbsp; &nbsp;datetimeFormat: ‘YYYY-MM-dd HH:mm:ss’&nbsp; &nbsp;};&nbsp; &nbsp;// Replace initialized params with provided params.&nbsp; &nbsp;if&nbsp;(params) {&nbsp; &nbsp; &nbsp; &nbsp;for&nbsp;(var&nbsp;param in&nbsp;params) {<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;_params[param] = params[param] || _params[param];<br>
&nbsp; &nbsp; &nbsp; &nbsp;}<br>
&nbsp; &nbsp;}&nbsp; &nbsp;// Set default parameters based on an image representative.&nbsp; &nbsp;var&nbsp;imgRep = ic.first();&nbsp; &nbsp;var&nbsp;nonSystemImgProps = ee.Feature(null)<br>
&nbsp; &nbsp; &nbsp; &nbsp;.copyProperties(imgRep).propertyNames();&nbsp; &nbsp;if&nbsp;(!_params.bands) _params.bands = imgRep.bandNames();&nbsp; &nbsp;if&nbsp;(!_params.bandsRename) _params.bandsRename = _params.bands;&nbsp; &nbsp;if&nbsp;(!_params.imgProps) _params.imgProps = nonSystemImgProps;&nbsp; &nbsp;if&nbsp;(!_params.imgPropsRename) _params.imgPropsRename = _params<br>
&nbsp; &nbsp; &nbsp; &nbsp;.imgProps;&nbsp; &nbsp;// Map the reduceRegions function over the image collection.&nbsp; &nbsp;var&nbsp;results = ic.map(function(img) {&nbsp; &nbsp; &nbsp; &nbsp;// Select bands (optionally rename), set a datetime &amp; timestamp property.&nbsp; &nbsp; &nbsp; &nbsp;img = ee.Image(img.select(_params.bands, _params<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.bandsRename))&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// Add datetime and timestamp features.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.set(_params.datetimeName, img.date().format(<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;_params.datetimeFormat))&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.set(‘timestamp’, img.get(‘system:time_start’));&nbsp; &nbsp; &nbsp; &nbsp;// Define final image property dictionary to set in output features.&nbsp; &nbsp; &nbsp; &nbsp;var&nbsp;propsFrom = ee.List(_params.imgProps)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.cat(ee.List([_params.datetimeName,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘timestamp’]));&nbsp; &nbsp; &nbsp; &nbsp;var&nbsp;propsTo =&nbsp;ee.List(_params.imgPropsRename)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.cat(ee.List([_params.datetimeName,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‘timestamp’]));&nbsp; &nbsp; &nbsp; &nbsp;var&nbsp;imgProps = img.toDictionary(propsFrom).rename(<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;propsFrom, propsTo);&nbsp; &nbsp; &nbsp; &nbsp;// Subset points that intersect the given image.&nbsp; &nbsp; &nbsp; &nbsp;var&nbsp;fcSub = fc.filterBounds(img.geometry());&nbsp; &nbsp; &nbsp; &nbsp;// Reduce the image by regions.&nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;img.reduceRegions({<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;collection: fcSub,<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;reducer: _params.reducer,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;scale: _params.scale,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;crs: _params.crs<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;})&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// Add metadata to each feature.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.map(function(f) {&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;f.set(imgProps);<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;});&nbsp; &nbsp; &nbsp; &nbsp;// Converts the feature collection of feature collections to a single&nbsp; &nbsp; &nbsp; &nbsp;//feature collection.&nbsp; &nbsp;}).flatten();&nbsp; &nbsp;return&nbsp;results;<br>
}</p>
</section>
</section>
<section id="point-collection-creation" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="point-collection-creation"><span class="header-section-number">9.3</span> Point Collection Creation</h2>
<p>Below, we create a set of points that form the basis of the zonal statistics calculations. Note that a unique plot_id&nbsp;property is added to each point. A unique plot or point ID is important to include in your vector dataset for future filtering and joining.</p>
<p>var&nbsp;pts = ee.FeatureCollection([&nbsp; &nbsp;ee.Feature(ee.Geometry.Point([-118.6010, 37.0777]), {<br>
&nbsp; &nbsp; &nbsp; &nbsp;plot_id: 1&nbsp; &nbsp;}),&nbsp; &nbsp;ee.Feature(ee.Geometry.Point([-118.5896, 37.0778]), {<br>
&nbsp; &nbsp; &nbsp; &nbsp;plot_id: 2&nbsp; &nbsp;}),&nbsp; &nbsp;ee.Feature(ee.Geometry.Point([-118.5842, 37.0805]), {<br>
&nbsp; &nbsp; &nbsp; &nbsp;plot_id: 3&nbsp; &nbsp;}),&nbsp; &nbsp;ee.Feature(ee.Geometry.Point([-118.5994, 37.0936]), {<br>
&nbsp; &nbsp; &nbsp; &nbsp;plot_id: 4&nbsp; &nbsp;}),&nbsp; &nbsp;ee.Feature(ee.Geometry.Point([-118.5861, 37.0567]), {<br>
&nbsp; &nbsp; &nbsp; &nbsp;plot_id: 5&nbsp; &nbsp;})<br>
]);print(‘Points of interest’, pts);</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F52a.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.</p>
</div>
</div>
</section>
<section id="neighborhood-statistic-examples" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="neighborhood-statistic-examples"><span class="header-section-number">9.4</span> Neighborhood Statistic Examples</h2>
<p>The following examples demonstrate extracting raster neighborhood statistics for the following:</p>
<ul>
<li>A single raster with elevation and slope bands</li>
<li>A multiband MODIS time series</li>
<li>A multiband Landsat time series</li>
</ul>
<p>In each example, the points created in the previous section will be buffered and then used as regions to extract zonal statistics for each image in the image collection.</p>
<section id="topographic-variables" class="level3" data-number="9.4.1">
<h3 data-number="9.4.1" class="anchored" data-anchor-id="topographic-variables"><span class="header-section-number">9.4.1</span> Topographic Variables</h3>
<p>This example demonstrates how to calculate zonal statistics for a single multiband image. This Digital Elevation Model (DEM) contains a single topographic band representing elevation.</p>
<p>###Buffer the Points</p>
<p>Nex, we will apply a 45 m radius buffer to the points defined previously by mapping&nbsp;the bufferPoints&nbsp;function over the feature collection. The radius is set to 45 m to correspond to the 90 m pixel resolution of the DEM. In this case, circles are used instead of squares (set the second argument as false, i.e., do not use bounds).</p>
<p>// Buffer the points.<br>
var&nbsp;ptsTopo = pts.map(bufferPoints(45, false));</p>
<p>###Calculate Zonal Statistics</p>
<p>There are two important things to note about the zonalStats&nbsp;function that this example addresses:</p>
<ul>
<li>It accepts only an ee.ImageCollection, not an ee.Image; single images must be wrapped in an ImageCollection.</li>
<li>It expects every image in the input image collection to have a timestamp property named ‘system:time_start’&nbsp;with values representing milliseconds from 00:00:00 UTC on 1 January 1970.&nbsp;Most datasets should have this property, if not, one should be added.</li>
</ul>
<p>// Import the MERIT global elevation dataset.<br>
var&nbsp;elev = ee.Image(‘MERIT/DEM/v1_0_3’);</p>
<p>// Calculate slope from the DEM.<br>
var&nbsp;slope = ee.Terrain.slope(elev);</p>
<p>// Concatenate elevation and slope as two bands of an image.<br>
var&nbsp;topo = ee.Image.cat(elev, slope)<br>
&nbsp; &nbsp; // Computed images do not have a ‘system:time_start’ property; add one based<br>
&nbsp; &nbsp; // on when the data were collected.&nbsp; &nbsp;.set(‘system:time_start’, ee.Date(‘2000-01-01’).millis());</p>
<p>// Wrap the single image in an ImageCollection for use in the<br>
// zonalStats function.<br>
var&nbsp;topoCol = ee.ImageCollection([topo]);</p>
<p>Define arguments for the zonalStats&nbsp;function and then run it. Note that we are accepting defaults for the reducer, scale, Coordinate Reference System (CRS), and image properties to copy over to the resulting feature collection. Refer to the function definition above for defaults.</p>
<p>// Define parameters for the zonalStats function.<br>
var&nbsp;params = {<br>
&nbsp; &nbsp;bands: [0, 1],<br>
&nbsp; &nbsp;bandsRename: [‘elevation’, ‘slope’]<br>
};</p>
<p>// Extract zonal statistics per point per image.<br>
var&nbsp;ptsTopoStats = zonalStats(topoCol, ptsTopo, params);print(‘Topo zonal stats table’, ptsTopoStats);</p>
<p>// Display the layers on the map.<br>
Map.setCenter(-118.5957, 37.0775, 13);<br>
Map.addLayer(topoCol.select(0), {<br>
&nbsp; &nbsp;min: 2400,<br>
&nbsp; &nbsp;max: 4200}, ‘Elevation’);<br>
Map.addLayer(topoCol.select(1), {<br>
&nbsp; &nbsp;min: 0,<br>
&nbsp; &nbsp;max: 60}, ‘Slope’);<br>
Map.addLayer(pts, {<br>
&nbsp; &nbsp;color: ‘purple’}, ‘Points’);<br>
Map.addLayer(ptsTopo, {<br>
&nbsp; &nbsp;color: ‘yellow’}, ‘Points w/ buffer’);</p>
<p>The result is a copy of the buffered point feature collection with new properties added for the region reduction of each selected image band according to the given reducer. A part of the FeatureCollection&nbsp;is shown in Fig. F5.2.1. The data in that FeatureCollection&nbsp;corresponds to a table containing the information of Table F5.2.3. See Fig. F5.2.2 for a graphical representation of the points and the topographic data being summarized.</p>
<p><img src="F5/image29.png" class="img-fluid"></p>
<p>Fig. F5.2.1&nbsp;A part of the FeatureCollection&nbsp;produced by calculating the zonal statistics</p>
<p><img src="F5/image5.png" class="img-fluid"></p>
<p>Fig. F5.2.2&nbsp;Sample points and topographic slope. Elevation and slope values for regions intersecting each buffered point are reduced and attached as properties of the points.</p>
<p>Table F5.2.3&nbsp;Example output from zonalStats&nbsp;organized as a table. Rows correspond to collection features and columns are feature properties. Note that elevation and slope values in this table are rounded to the nearest tenth for brevity.</p>
<p>plot_id</p>
<p>timestamp</p>
<p>datetime</p>
<p>elevation</p>
<p>slope</p>
<p>1</p>
<p>946684800000</p>
<p>2000-01-01 00:00:00</p>
<p>2648.1</p>
<p>29.7</p>
<p>2</p>
<p>946684800000</p>
<p>2000-01-01 00:00:00</p>
<p>2888.2</p>
<p>33.9</p>
<p>3</p>
<p>946684800000</p>
<p>2000-01-01 00:00:00</p>
<p>3267.8</p>
<p>35.8</p>
<p>4</p>
<p>946684800000</p>
<p>2000-01-01 00:00:00</p>
<p>2790.7</p>
<p>25.1</p>
<p>5</p>
<p>946684800000</p>
<p>2000-01-01 00:00:00</p>
<p>2559.4</p>
<p>29.4</p>
</section>
<section id="modis-time-series" class="level3" data-number="9.4.2">
<h3 data-number="9.4.2" class="anchored" data-anchor-id="modis-time-series"><span class="header-section-number">9.4.2</span> MODIS Time Series</h3>
<p>A time series of MODIS eight-day surface reflectance composites demonstrates how to calculate zonal statistics for a multiband ImageCollection&nbsp;that requires no preprocessing, such as cloud masking or computation. Note that there is no built-in function for performing region reductions on ImageCollection&nbsp;objects. The zonalStats&nbsp;function that we are using for reduction is mapping&nbsp;the reduceRegions&nbsp;function over an ImageCollection.</p>
<p>###Buffer the Points</p>
<p>In this example, suppose the point collection represents center points for field plots that are 100 m x 100 m, and apply a 50 m radius buffer to the points to match the size of the plot. Since we want zonal statistics for square plots, set the second argument of the bufferPoints&nbsp;function to true, so that the bounds of the buffered points are returned.</p>
<p>var&nbsp;ptsModis = pts.map(bufferPoints(50, true));</p>
<p>###Calculate Zonal Statistic</p>
<p>Import the MODIS 500 m global eight-day surface reflectance composite collection and filter the collection to include data for July, August, and September from 2015 through 2019.</p>
<p>var&nbsp;modisCol = ee.ImageCollection(‘MODIS/006/MOD09A1’)<br>
&nbsp; &nbsp;.filterDate(‘2015-01-01’, ‘2020-01-01’)<br>
&nbsp; &nbsp;.filter(ee.Filter.calendarRange(183, 245, ‘DAY_OF_YEAR’));</p>
<p>Reduce each image in the collection by each plot according to the following parameters. Note that this time the reducer is defined as the neighborhood median (ee.Reducer.median) instead of the default mean, and that scale, CRS, and properties for the datetime are explicitly defined.</p>
<p>// Define parameters for the zonalStats function.<br>
var&nbsp;params = {<br>
&nbsp; &nbsp;reducer: ee.Reducer.median(),<br>
&nbsp; &nbsp;scale: 500,<br>
&nbsp; &nbsp;crs: ‘EPSG:5070’,<br>
&nbsp; &nbsp;bands: [‘sur_refl_b01’, ‘sur_refl_b02’, ‘sur_refl_b06’],<br>
&nbsp; &nbsp;bandsRename: [‘modis_red’, ‘modis_nir’, ‘modis_swir’],<br>
&nbsp; &nbsp;datetimeName: ‘date’,<br>
&nbsp; &nbsp;datetimeFormat: ‘YYYY-MM-dd’<br>
};</p>
<p>// Extract zonal statistics per point per image.<br>
var&nbsp;ptsModisStats = zonalStats(modisCol, ptsModis, params);print(‘Limited MODIS zonal stats table’, ptsModisStats.limit(50));</p>
<p>The result is a feature collection with a feature for all combinations of plots and images. Interpreted as a table, the result has 200 rows (5 plots times 40 images) and as many columns as there are feature properties. Feature properties include those from the plot asset and the image, and any associated non-system image properties. Note that the printed results are limited to the first 50 features for brevity.</p>
</section>
<section id="landsat-time-series" class="level3" data-number="9.4.3">
<h3 data-number="9.4.3" class="anchored" data-anchor-id="landsat-time-series"><span class="header-section-number">9.4.3</span> Landsat Time Series</h3>
<p>This example combines Landsat surface reflectance imagery across three instruments: Thematic Mapper (TM) from Landsat 5, Enhanced Thematic Mapper Plus (ETM+) from Landsat 7, and Operational Land Imager (OLI) from Landsat 8.</p>
<p>The following section prepares these collections so that band names are consistent and cloud masks are applied. Reflectance among corresponding bands are roughly congruent for the three sensors when using the surface reflectance product; therefore the processing steps that follow do not address inter-sensor harmonization.&nbsp;Review the current literature on inter-sensor harmonization practices if you’d like to apply a correction.</p>
<p>###Prepare the Landsat Image Collection</p>
<p>First, define the function to mask cloud and shadow pixels&nbsp;(See Chap. F4.3&nbsp;for more detail on cloud masking).</p>
<p>// Mask clouds from images and apply scaling factors.<br>
function&nbsp;maskScale(img) {&nbsp; &nbsp;var&nbsp;qaMask = img.select(‘QA_PIXEL’).bitwiseAnd(parseInt(‘11111’,&nbsp; &nbsp; &nbsp; &nbsp;2)).eq(0);&nbsp; &nbsp;var&nbsp;saturationMask = img.select(‘QA_RADSAT’).eq(0);&nbsp; &nbsp;// Apply the scaling factors to the appropriate bands.&nbsp; &nbsp;var&nbsp;getFactorImg = function(factorNames) {&nbsp; &nbsp; &nbsp; &nbsp;var&nbsp;factorList = img.toDictionary().select(factorNames)<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.values();&nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;ee.Image.constant(factorList);<br>
&nbsp; &nbsp;};&nbsp; &nbsp;var&nbsp;scaleImg = getFactorImg([‘REFLECTANCE_MULT_BAND_.’]);&nbsp; &nbsp;var&nbsp;offsetImg = getFactorImg([‘REFLECTANCE_ADD_BAND_.’]);&nbsp; &nbsp;var&nbsp;scaled = img.select(‘SR_B.’).multiply(scaleImg).add(<br>
&nbsp; &nbsp;offsetImg);&nbsp; &nbsp;// Replace the original bands with the scaled ones and apply the masks.&nbsp; &nbsp;return&nbsp;img.addBands(scaled, null, true)<br>
&nbsp; &nbsp; &nbsp; &nbsp;.updateMask(qaMask)<br>
&nbsp; &nbsp; &nbsp; &nbsp;.updateMask(saturationMask);<br>
}</p>
<p>Next, define functions to select and rename the bands of interest for the Operational Land Imager (OLI) aboard Landsat 8, and for the TM/ETM+ imagers aboard earlier Landsats. This is important because the band numbers are different for OLI and TM/ETM+, and it will make future index calculations easier.</p>
<p>// Selects and renames bands of interest for Landsat OLI.<br>
function&nbsp;renameOli(img) {&nbsp; &nbsp;return&nbsp;img.select(<br>
&nbsp; &nbsp; &nbsp; &nbsp;[‘SR_B2’, ‘SR_B3’, ‘SR_B4’, ‘SR_B5’, ‘SR_B6’, ‘SR_B7’],<br>
&nbsp; &nbsp; &nbsp; &nbsp;[‘Blue’, ‘Green’, ‘Red’, ‘NIR’, ‘SWIR1’, ‘SWIR2’]);<br>
}</p>
<p>// Selects and renames bands of interest for TM/ETM+.<br>
function&nbsp;renameEtm(img) {&nbsp; &nbsp;return&nbsp;img.select(<br>
&nbsp; &nbsp; &nbsp; &nbsp;[‘SR_B1’, ‘SR_B2’, ‘SR_B3’, ‘SR_B4’, ‘SR_B5’, ‘SR_B7’],<br>
&nbsp; &nbsp; &nbsp; &nbsp;[‘Blue’, ‘Green’, ‘Red’, ‘NIR’, ‘SWIR1’, ‘SWIR2’]);<br>
}</p>
<p>Combine the cloud mask and band renaming functions into preparation functions for OLI and TM/ETM+. Add any other sensor-specific preprocessing steps that you’d like to the functions below.</p>
<p>// Prepares (cloud masks and renames) OLI images.<br>
function&nbsp;prepOli(img) {<br>
&nbsp; &nbsp;img = maskScale(img);<br>
&nbsp; &nbsp;img = renameOli(img);&nbsp; &nbsp;return&nbsp;img;<br>
}// Prepares (cloud masks and renames) TM/ETM+ images.<br>
function&nbsp;prepEtm(img) {<br>
&nbsp; &nbsp;img = maskScale(img);<br>
&nbsp; &nbsp;img = renameEtm(img);&nbsp; &nbsp;return&nbsp;img;<br>
}</p>
<p>Get the Landsat surface reflectance collections for OLI, ETM+, and TM sensors. Filter them by the bounds of the point feature collection and apply the relevant image preparation function.</p>
<p>var&nbsp;ptsLandsat = pts.map(bufferPoints(15, true));</p>
<p>var&nbsp;oliCol = ee.ImageCollection(‘LANDSAT/LC08/C02/T1_L2’)<br>
&nbsp; &nbsp;.filterBounds(ptsLandsat)<br>
&nbsp; &nbsp;.map(prepOli);</p>
<p>var&nbsp;etmCol = ee.ImageCollection(‘LANDSAT/LE07/C02/T1_L2’)<br>
&nbsp; &nbsp;.filterBounds(ptsLandsat)<br>
&nbsp; &nbsp;.map(prepEtm);</p>
<p>var&nbsp;tmCol = ee.ImageCollection(‘LANDSAT/LT05/C02/T1_L2’)<br>
&nbsp; &nbsp;.filterBounds(ptsLandsat)<br>
&nbsp; &nbsp;.map(prepEtm);</p>
<p>Merge the prepared sensor collections.</p>
<p>var&nbsp;landsatCol = oliCol.merge(etmCol).merge(tmCol);</p>
<p>###Calculate Zonal Statistics</p>
<p>Reduce each image in the collection by each plot according to the following parameters. Note that this example defines the imgProps&nbsp;and imgPropsRename&nbsp;parameters to copy over and rename just two selected image properties: Landsat image ID and the satellite that collected the data. It also uses the max&nbsp;reducer, which, as an unweighted reducer, will return the maximum value from pixels that have their centroid within the buffer (see Sect. 4.1 below for more details).</p>
<p>// Define parameters for the zonalStats function.<br>
var&nbsp;params = {<br>
&nbsp; &nbsp;reducer: ee.Reducer.max(),<br>
&nbsp; &nbsp;scale: 30,<br>
&nbsp; &nbsp;crs: ‘EPSG:5070’,<br>
&nbsp; &nbsp;bands: [‘Blue’, ‘Green’, ‘Red’, ‘NIR’, ‘SWIR1’, ‘SWIR2’],<br>
&nbsp; &nbsp;bandsRename: [‘ls_blue’, ‘ls_green’, ‘ls_red’, ‘ls_nir’,&nbsp; &nbsp; &nbsp; &nbsp;‘ls_swir1’, ‘ls_swir2’&nbsp; &nbsp;],<br>
&nbsp; &nbsp;imgProps: [‘SENSOR_ID’, ‘SPACECRAFT_ID’],<br>
&nbsp; &nbsp;imgPropsRename: [‘img_id’, ‘satellite’],<br>
&nbsp; &nbsp;datetimeName: ‘date’,<br>
&nbsp; &nbsp;datetimeFormat: ‘YYYY-MM-dd’<br>
};</p>
<p>// Extract zonal statistics per point per image.<br>
var&nbsp;ptsLandsatStats = zonalStats(landsatCol, ptsLandsat, params)&nbsp; &nbsp;// Filter out observations where image pixels were all masked.&nbsp; &nbsp;.filter(ee.Filter.notNull(params.bandsRename));<br>
print(‘Limited Landsat zonal stats table’, ptsLandsatStats.limit(50));</p>
<p>The result is a feature collection with a feature for all combinations of plots and images.</p>
<p>###Dealing with Large Collections</p>
<p>If your browser times out, try exporting the results&nbsp;(as described in Chap. F6.2). It’s likely that point feature collections that cover a large area or contain many points (point-image observations) will need to be exported as a batch task by either exporting the final feature collection as an asset or as a CSV/shapefile/GeoJSON to Google Drive or GCS.</p>
<p>Here is how you would export the above Landsat image-point feature collection to an asset and to Google Drive. Run the following code, activate the Code Editor Tasks&nbsp;tab, and then click the Run&nbsp;button. If you don’t specify your own existing folder in Drive, the folder “EEFA_outputs” will be created.</p>
<p>Export.table.toAsset({<br>
&nbsp; &nbsp;collection: ptsLandsatStats,<br>
&nbsp; &nbsp;description: ‘EEFA_export_Landsat_to_points’,<br>
&nbsp; &nbsp;assetId: ‘EEFA_export_values_to_points’<br>
});</p>
<p>Export.table.toDrive({<br>
&nbsp; &nbsp;collection: ptsLandsatStats,<br>
&nbsp; &nbsp;folder: ‘EEFA_outputs’, // this will create a new folder if it doesn’t exist&nbsp; &nbsp;description: ‘EEFA_export_values_to_points’,<br>
&nbsp; &nbsp;fileFormat: ‘CSV’<br>
});</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F52b.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.</p>
</div>
</div>
</section>
</section>
<section id="additional-notes" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="additional-notes"><span class="header-section-number">9.5</span> Additional Notes</h2>
<section id="weighted-versus-unweighted-region-reduction" class="level3" data-number="9.5.1">
<h3 data-number="9.5.1" class="anchored" data-anchor-id="weighted-versus-unweighted-region-reduction"><span class="header-section-number">9.5.1</span> Weighted Versus Unweighted Region Reduction</h3>
<p>A region used for calculation of zonal statistics often bisects multiple pixels. Should partial pixels be included in zonal statistics? Earth Engine lets you decide by allowing you to define a reducer as either weighted or unweighted (or you can provide per-pixel weight specification as an image band). A weighted&nbsp;reducer will include partial pixels in the zonal statistic&nbsp;calculation by weighting each pixel’s contribution according to the fraction of the area intersecting the region. An unweighted&nbsp;reducer, on the other hand, gives equal weight to all pixels whose cell center intersects the region; all other pixels are excluded from calculation of the statistic.</p>
<p>For aggregate reducers like ee.Reducer.mean&nbsp;and ee.Reducer.median, the default mode is weighted, while identifier reducers such as ee.Reducer.min&nbsp;and ee.Reducer.max&nbsp;are unweighted. You can adjust the behavior of weighted reducers by calling unweighted&nbsp;on them, as in ee.Reducer.mean.unweighted. You may also specify the weights by modifying the reducer with splitWeights; however, that is beyond the scope of this book.</p>
</section>
<section id="copy-properties-to-computed-images" class="level3" data-number="9.5.2">
<h3 data-number="9.5.2" class="anchored" data-anchor-id="copy-properties-to-computed-images"><span class="header-section-number">9.5.2</span> Copy Properties to Computed Images</h3>
<p>Derived, computed images do not retain the properties of their source image, so be sure to copy properties to computed images if you want them included in the region reduction table. For instance, consider the simple computation of unscaling Landsat SR data:</p>
<p>// Define a Landsat image.<br>
var&nbsp;img = ee.ImageCollection(‘LANDSAT/LC08/C02/T1_L2’).first();</p>
<p>// Print its properties.<br>
print(‘All image properties’, img.propertyNames());</p>
<p>// Subset the reflectance bands and unscale them.<br>
var&nbsp;computedImg = img.select(‘SR_B.’).multiply(0.0000275).add(-0.2);</p>
<p>// Print the unscaled image’s properties.<br>
print(‘Lost original image properties’, computedImg.propertyNames());</p>
<p>Notice how the computed image does not have the source image’s properties and only retains the bands information. To fix this, use the copyProperties&nbsp;function to add desired source properties to the derived image. It is best practice to copy only the properties you really need because&nbsp;some properties, such as those containing geometry objects, lists, or feature collections, can significantly&nbsp;increase the computational burden for large collections.</p>
<p>// Subset the reflectance bands and unscale them, keeping selected<br>
// source properties.<br>
var&nbsp;computedImg = img.select(‘SR_B.’).multiply(0.0000275).add(-0.2)<br>
&nbsp; &nbsp;.copyProperties(img, [‘system:time_start’, ‘LANDSAT_PRODUCT_ID’]);</p>
<p>// Print the unscaled image’s properties.<br>
print(‘Selected image properties retained’, computedImg<br>
.propertyNames());</p>
<p>Now selected properties are included. Use this technique when returning computed, derived images in a mapped function, and in single-image operations.</p>
</section>
<section id="understanding-which-pixels-are-included-in-polygon-statistics" class="level3" data-number="9.5.3">
<h3 data-number="9.5.3" class="anchored" data-anchor-id="understanding-which-pixels-are-included-in-polygon-statistics"><span class="header-section-number">9.5.3</span> Understanding Which Pixels are Included in Polygon Statistics</h3>
<p>If you want to visualize what pixels are included in a polygon for a region reducer, you can adapt the following code to use your own region (by replacing geometry), dataset, desired scale, and CRS parameters. The important part to note is that the image data you are adding to the map is reprojected using the same scale and CRS as that used in your region reduction (see Fig. F5.2.3).</p>
<p>// Define polygon geometry.<br>
var&nbsp;geometry = ee.Geometry.Polygon(<br>
&nbsp; &nbsp;[<br>
&nbsp; &nbsp; &nbsp; &nbsp;[<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[-118.6019835717645, 37.079867782687884],<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[-118.6019835717645, 37.07838698844939],<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[-118.60036351751951, 37.07838698844939],<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[-118.60036351751951, 37.079867782687884]<br>
&nbsp; &nbsp; &nbsp; &nbsp;]<br>
&nbsp; &nbsp;], null, false);</p>
<p>// Import the MERIT global elevation dataset.<br>
var&nbsp;elev = ee.Image(‘MERIT/DEM/v1_0_3’);</p>
<p>// Define desired scale and crs for region reduction (for image display too).<br>
var&nbsp;proj = {<br>
&nbsp; &nbsp;scale: 90,<br>
&nbsp; &nbsp;crs: ‘EPSG:5070’<br>
};</p>
<p>The count&nbsp;reducer will return how many pixel centers are overlapped by the polygon region, which would be the number of pixels included in any unweighted reducer statistic. You can also visualize which pixels will be included in the reduction by using the toCollection&nbsp;reducer on a&nbsp;latitude/longitude image and adding resulting coordinates as feature geometry. Be sure to specify CRS and scale for both the region reducers and the reprojected layer added to the map (see bullet list below for more details).</p>
<p>// A count reducer will return how many pixel centers are overlapped by the<br>
// polygon region.<br>
var&nbsp;count = elev.select(0).reduceRegion({<br>
&nbsp; &nbsp;reducer: ee.Reducer.count(),<br>
&nbsp; &nbsp;geometry: geometry,<br>
&nbsp; &nbsp;scale: proj.scale,&nbsp; &nbsp;crs: proj.crs<br>
});<br>
print(‘n pixels in the reduction’, count.get(‘dem’));</p>
<p>// Make a feature collection of pixel center points for those that are<br>
// included in the reduction.<br>
var&nbsp;pixels = ee.Image.pixelLonLat().reduceRegion({<br>
&nbsp; &nbsp;reducer: ee.Reducer.toCollection([‘lon’, ‘lat’]),<br>
&nbsp; &nbsp;geometry: geometry,<br>
&nbsp; &nbsp; scale: proj.scale,&nbsp; &nbsp;crs: proj.crs<br>
});<br>
var&nbsp;pixelsFc = ee.FeatureCollection(pixels.get(‘features’)).map(&nbsp; &nbsp;function(f) {&nbsp; &nbsp; &nbsp; &nbsp;return&nbsp;f.setGeometry(ee.Geometry.Point([f.get(‘lon’), f<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.get(‘lat’)<br>
&nbsp; &nbsp; &nbsp; &nbsp;]));<br>
&nbsp; &nbsp;});</p>
<p>// Display layers on the map.<br>
Map.centerObject(geometry, 18);<br>
Map.addLayer(<br>
&nbsp; &nbsp;elev.reproject({<br>
&nbsp; &nbsp; &nbsp; &nbsp;crs: proj.crs,<br>
&nbsp; &nbsp; &nbsp; &nbsp;scale: proj.scale&nbsp; &nbsp;}),<br>
&nbsp; &nbsp;{<br>
&nbsp; &nbsp; &nbsp; &nbsp;min: 2500,<br>
&nbsp; &nbsp; &nbsp; &nbsp;max: 3000,<br>
&nbsp; &nbsp; &nbsp; &nbsp;palette: [‘blue’, ‘white’, ‘red’]<br>
&nbsp; &nbsp;}, ‘Image’);<br>
Map.addLayer(geometry, {<br>
&nbsp; &nbsp;color: ‘white’}, ‘Geometry’);<br>
Map.addLayer(pixelsFc, {<br>
&nbsp; &nbsp;color: ‘purple’}, ‘Pixels in reduction’);</p>
<p><img src="F5/image44.png" class="img-fluid"></p>
<p>Fig. F5.2.3&nbsp;Identifying pixels used in zonal statistics. By mapping the image and vector together, you can see which pixels are included in the unweighted statistic. For this example, three pixels would be included in the statistic because the polygon covers the center point of three pixels.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F52c.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.</p>
</div>
</div>
<p>Finally, here are some notes on CRS and scale:</p>
<ul>
<li>Earth Engine runs reduceRegion&nbsp;using the projection of the image’s first band if the CRS is unspecified in the function. For imagery spanning multiple UTM zones, for example, this would lead to different origins. For some functions Earth Engine&nbsp;uses the default EPSG:4326. Therefore, when the opportunity is presented, such as by the reduceRegion&nbsp;function, it is important to specify the scale and CRS explicitly.</li>
<li>The Map&nbsp;default CRS is EPSG:3857. When looking closely at pixels on the map, the data layer scale and CRS should also be set explicitly. Note that zooming out after setting a relatively small scale when reprojecting may result in memory and/or timeout errors because optimized pyramid layers for each zoom level will not be used.</li>
<li>Specifying the CRS and scale in both the reduceRegion&nbsp;and addLayer&nbsp;functions allows the map visualization to align with the information printed in the Console.</li>
<li>The Earth Engine default, WGS 84 lat long (EPSG:4326), is a generic CRS that works worldwide. The code above reprojects to EPSG:5070, North American Equal Albers, which is a CRS that preserves area for North American locations. Use the CRS that is best for your use case when adapting this to your own project, or maintain (and specify) the CRS of the image using, for example, crs: ‘img.projection().crs()’.</li>
</ul>
</section>
</section>
<section id="synthesis-2" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="synthesis-2">Synthesis</h2>
<p>Question 1.&nbsp;Look at the MODIS example (Sect. 3.2), which uses the median reducer. Try modifying the reducer to be unweighted, either by specifying unweighted&nbsp;or using an identifier reducer like max. What happens, and why?</p>
<p>Question 2. Calculate zonal statistics for your own buffered points or polygons using a raster and reducer of interest. Be sure to consider the spatial scale of the raster and whether a weighted or unweighted reducer would be more appropriate for your interests.</p>
<p>If the point or polygon file is stored in a local shapefile or CSV file, first upload the data to your Earth Engine assets. All columns in your vector file, such as the plot name, will be retained through this process. Once you have an Earth Engine table asset ready, import the asset into your script by hovering over the name of the asset and clicking the arrow at the right side, or by calling it in your script with the following code.</p>
<p>var&nbsp;pts = ee.FeatureCollection(‘users/yourUsername/yourAsset’);</p>
<p>If you prefer to define points or polygons dynamically rather than loading an asset, you can add them to your script using the geometry tools. See Chap. F2.1 and F5.0 for more detail on adding and creating vector data.</p>
<p>Question 3.&nbsp;Try the code from&nbsp;Sect. 4.3 using&nbsp;the MODIS data and the first point from the pts&nbsp;variable. Among other modifications, you will need to create a buffer for the point, take a single MODIS image from the collection, and change visualization parameters.</p>
<ul>
<li>Think about the CRS in the code: The code reprojects to EPSG:5070, but MODIS is collected in the sinusoidal projection SR-ORG:6974. Try that CRS and describe how the image changes.</li>
<li>Is the count reducer weighted or unweighted? Give an example of a circumstance to use a weighted reducer and an example for an unweighted reducer. Specify the buffer size you would use and the spatial resolution of your dataset.</li>
</ul>
<p>Question 4.&nbsp;In the examples above, only a single ee.Reducer&nbsp;is passed to the zonalStats&nbsp;function, which means that only a single statistic is calculated (for example, zonal mean or median or maximum). What if you want multiple statistics—can you alter the code in&nbsp;Sect. 3.1&nbsp;to (1) make the point buffer 500 instead of 45; (2) add the reducer&nbsp;parameter to the params&nbsp;dictionary; and (3) as its argument, supply a combined ee.Reducer&nbsp;that will calculate minimum, maximum, standard deviation, and mean statistics?</p>
<p>To achieve this you’ll need to chain several ee.Reducer.combine&nbsp;functions together. Note that if you accept all the individual ee.Reducer&nbsp;and ee.Reducer.combine&nbsp;function defaults, you’ll run into two problems related to reducer weighting differences, and whether or not the image inputs are shared among the combined set of reducers. How can you manipulate the individual ee.Reducer&nbsp;and ee.Reducer.combine&nbsp;functions to achieve the goal of calculating multiple zonal statistics in one call to the zonalStats&nbsp;function?</p>
</section>
<section id="conclusion-2" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="conclusion-2">Conclusion</h2>
<p>In this chapter, you used functions containing optional parameters to extract raster values for collocated points. You also learned how to buffer points, and apply weighted and unweighted reducers to get different types of zonal statistics. These functions were applied to three examples that differed by raster dataset, reducer, spatial resolution, and scale. Lastly, you covered related topics like weighting of reducers and buffer visualization. Now you’re ready to apply these ideas to your own work!</p>
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>
<p>Cansler CA, McKenzie D (2012) How robust are burn severity indices when applied in a new region? Evaluation of alternate field-based and remote-sensing methods. Remote Sens 4:456–483. https://doi.org/10.3390/rs4020456</p>
<p>Miller JD, Thode AE (2007) Quantifying burn severity in a heterogeneous landscape with a relative version of the delta Normalized Burn Ratio (dNBR). Remote Sens Environ 109:66–80. https://doi.org/10.1016/j.rse.2006.12.006</p>
</section>
</section>
<section id="advanced-vector-operations" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Advanced Vector Operations</h1>
<p>::: {.callout-tip} # Chapter Information</p>
<section id="author-3" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="author-3">Author</h2>
<p>&nbsp;</p>
<p>Ujaval Gandhi</p>
</section>
<section id="overview-2" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="overview-2">Overview</h2>
<p>&nbsp;</p>
<p>This chapter covers advanced techniques for visualizing and analyzing vector data in Earth Engine. There are many ways to visualize feature collections, and you will learn how to pick the appropriate method to create visualizations, such as a choropleth map. We will also cover geoprocessing techniques involving multiple vector layers, such as selecting features in one layer by their proximity to features in another layer and performing spatial joins.</p>
</section>
<section id="learning-outcomes-2" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="learning-outcomes-2">Learning Outcomes</h2>
<ul>
<li>Visualizing any vector dataset and creating a thematic map.</li>
<li>Understanding joins in Earth Engine.</li>
<li>Carrying out geoprocessing tasks with vector layers in Earth Engine.</li>
</ul>
</section>
<section id="assumes-you-know-how-to-2" class="level2 unlisted unnumbered">
<h2 class="unlisted unnumbered anchored" data-anchor-id="assumes-you-know-how-to-2">Assumes you know how to:</h2>
<ul>
<li>Filter a FeatureCollection&nbsp;to obtain a subset (Chap. F5.0, Chap. F5.1).</li>
<li>Write a function and map&nbsp;it over a FeatureCollection&nbsp;(Chap. F5.1, Chap. F5.2).</li>
</ul>
</section>
<section id="visualizing-feature-collections" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="visualizing-feature-collections"><span class="header-section-number">10.1</span> Visualizing Feature Collections</h2>
<p>There is a distinct difference between how rasters and vectors are visualized. While images are typically visualized based on pixel values, vector layers use feature properties (i.e., attributes) to create a visualization. Vector layers are rendered on the Map&nbsp;by assigning a value to the red, green, and blue channels for each pixel on the screen based on the geometry and attributes of the features. The functions used for vector data visualization in Earth Engine are listed below in increasing order of complexity.</p>
<ul>
<li>Map.addLayer: As with raster layers, you can add a FeatureCollection&nbsp;to the Map&nbsp;by specifying visualization parameters. This method supports only one visualization parameter: color. All features are rendered with the specified color.</li>
<li>draw: This function supports the parameters pointRadius&nbsp;and strokeWidth&nbsp;in addition to color. It renders all features of the layer with the specified parameters.</li>
<li>paint: This is a more powerful function that can render each feature with a different color&nbsp;and width&nbsp;based on the values in the specified property.</li>
<li>style: This is the most versatile function. It can apply a different style to each feature, including color, pointSize, pointShape, width, fillColor, and lineType.</li>
</ul>
<p>In the exercises below, we will learn how to use each of these functions and see how they can generate different types of maps.</p>
<section id="creating-a-choropleth-map" class="level3" data-number="10.1.1">
<h3 data-number="10.1.1" class="anchored" data-anchor-id="creating-a-choropleth-map"><span class="header-section-number">10.1.1</span> Creating a Choropleth Map</h3>
<p>We will use the&nbsp;TIGER: US Census Blocks&nbsp;layer, which stores census block boundaries and their characteristics within the United States, along with the San Francisco neighborhoods layer from Chap. F5.0 to create a population density map for the city of San Francisco.</p>
<p>We start by loading the census blocks and San Francisco neighborhoods layers. We use ee.Filter.bounds&nbsp;to filter the census blocks layer to the San Francisco boundary.</p>
<p>var&nbsp;blocks = ee.FeatureCollection(‘TIGER/2010/Blocks’);<br>
var&nbsp;roads = ee.FeatureCollection(‘TIGER/2016/Roads’);<br>
var&nbsp;sfNeighborhoods = ee.FeatureCollection(&nbsp; &nbsp;‘projects/gee-book/assets/F5-0/SFneighborhoods’);</p>
<p>var&nbsp;geometry = sfNeighborhoods.geometry();<br>
Map.centerObject(geometry);</p>
<p>// Filter blocks to the San Francisco boundary.<br>
var&nbsp;sfBlocks = blocks.filter(ee.Filter.bounds(geometry));</p>
<p>The simplest way to visualize this layer is to use Map.addLayer&nbsp;(Fig. F5.3.1). We can specify a color&nbsp;value in the visParams&nbsp;parameter of the function. Each census block polygon will be rendered with stroke and fill of the specified color. The fill color is the same as the stroke color but has a 66% opacity.</p>
<p>// Visualize with a single color.<br>
Map.addLayer(sfBlocks, {<br>
&nbsp; &nbsp;color: ‘#de2d26’}, ‘Census Blocks (single color)’);</p>
<p><img src="F5/image34.png" class="img-fluid"></p>
<p>Fig. F5.3.1&nbsp;San Francisco census blocks</p>
<p>The census blocks table has a property named ‘pop10’&nbsp;containing the population totals as of the 2010 census. We can use this to create a choropleth map showing population density. We first need to compute the population density for each feature and add it as a property. To add a new property to each feature, we can map&nbsp;a function&nbsp;over the FeatureCollection&nbsp;and calculate the new property called ‘pop_density’. Earth Engine provides the area&nbsp;function, which can calculate the area of a feature in square meters.&nbsp;We convert it to square miles and calculate the population density per square mile.</p>
<p>// Add a pop_density column.<br>
var&nbsp;sfBlocks = sfBlocks.map(function(f) {&nbsp; &nbsp;// Get the polygon area in square miles.&nbsp; &nbsp;var&nbsp;area_sqmi = f.area().divide(2.59e6);&nbsp; &nbsp;var&nbsp;population = f.get(‘pop10’);&nbsp; &nbsp;// Calculate population density.&nbsp; &nbsp;var&nbsp;density = ee.Number(population).divide(area_sqmi);&nbsp; &nbsp;return&nbsp;f.set({&nbsp; &nbsp; &nbsp; &nbsp;‘area_sqmi’: area_sqmi,&nbsp; &nbsp; &nbsp; &nbsp;‘pop_density’: density<br>
&nbsp; &nbsp;});<br>
});</p>
<p>Now we can use the paint&nbsp;function to create an image from this FeatureCollection&nbsp;using the pop_density&nbsp;property. The paint&nbsp;function needs an empty image that needs to be cast to the appropriate data type. Let’s use the aggregate_stats&nbsp;function to calculate basic statistics for the given column of a FeatureCollection.</p>
<p>// Calculate the statistics of the newly computed column.<br>
var&nbsp;stats = sfBlocks.aggregate_stats(‘pop_density’);<br>
print(stats);</p>
<p>You will see that the population density values have a large range. We also have values that are greater than 100,000, so we need to make sure we select a data type that can store values of this size. We create an empty image and cast it to int32, which is able to hold large integer values.</p>
<p>D</p>
<p>The result is an image with pixel values representing the population density of the polygons. We can now use the standard image visualization method to add this layer to the Map&nbsp;(Fig. F5.3.2). Then, we need to determine minimum and maximum values for the visualization parameters.A reliable technique to produce a good visualization is to find minimum and maximum values that are within one standard deviation. From the statistics that we calculated earlier, we can estimate good minimum and maximum values to be 0 and 50000, respectively.</p>
<p>var&nbsp;palette = [‘fee5d9’, ‘fcae91’, ‘fb6a4a’, ‘de2d26’, ‘a50f15’];<br>
var&nbsp;visParams = {<br>
&nbsp; &nbsp;min: 0,<br>
&nbsp; &nbsp;max: 50000,<br>
&nbsp; &nbsp;palette: palette<br>
};<br>
Map.addLayer(sfBlocksPaint.clip(geometry), visParams,&nbsp; &nbsp;‘Population Density’);</p>
<p><img src="F5/image41.png" class="img-fluid"></p>
<p>Fig. F5.3.2&nbsp;San Francisco population density</p>
</section>
<section id="creating-a-categorical-map" class="level3" data-number="10.1.2">
<h3 data-number="10.1.2" class="anchored" data-anchor-id="creating-a-categorical-map"><span class="header-section-number">10.1.2</span> Creating a Categorical Map</h3>
<p>Continuing the exploration of styling methods, we will now learn about draw&nbsp;and style. These are the preferred methods of styling for points and line layers. Let’s see how we can visualize the&nbsp;TIGER: US Census Roads&nbsp;layer to create a categorical map.</p>
<p>We start by filtering the roads layer to the San Francisco boundary and using Map.addLayer&nbsp;to visualize it.</p>
<p>// Filter roads to San Francisco boundary.<br>
var&nbsp;sfRoads = roads.filter(ee.Filter.bounds(geometry));</p>
<p>Map.addLayer(sfRoads, {<br>
&nbsp; &nbsp;color: ‘blue’}, ‘Roads (default)’);</p>
<p>The default visualization renders each line using a width of 2 pixels. The draw&nbsp;function provides a way to specify a different line width. Let’s use it to render the layer with the same color as before but with a line width of 1 pixel (Fig. F5.3.3).</p>
<p>// Visualize with draw().<br>
var&nbsp;sfRoadsDraw = sfRoads.draw({<br>
&nbsp; &nbsp;color: ‘blue’,<br>
&nbsp; &nbsp;strokeWidth: 1<br>
});<br>
Map.addLayer(sfRoadsDraw, {}, ‘Roads (Draw)’);</p>
<p><img src="F5/image28.png" class="img-fluid"></p>
<p><img src="F5/image31.png" class="img-fluid"></p>
<p>Fig. F5.3.3&nbsp;San Francisco roads rendered with a line width of 2 pixels (left) and and a line width of 1 pixel (right)</p>
<p>The road layer has a column called “MTFCC” (standing for the MAF/TIGER Feature Class Code). This contains the road priority codes, representing the various types of roads, such as primary and secondary. We can use this information to render each road segment according to its priority. The draw&nbsp;function doesn’t allow us to specify different styles for each feature. Instead, we need to make use of the style&nbsp;function.</p>
<p>The column contains string values indicating different road types as indicated in Table F5.3.1. This full list is available at the&nbsp;MAF/TIGER Feature Class Code Definitions&nbsp;page on the US Census Bureau website.</p>
<p>Table F5.3.1&nbsp;Census Bureau road priority codes</p>
<p>MTFCC</p>
<p>Feature Class</p>
<p>S1100</p>
<p>Primary Road</p>
<p>S1200</p>
<p>Secondary Road</p>
<p>S1400</p>
<p>Local Neighborhood Road, Rural Road, City Street</p>
<p>S1500</p>
<p>Vehicular Trail</p>
<p>S1630</p>
<p>Ramp</p>
<p>S1640</p>
<p>Service Drive</p>
<p>S1710</p>
<p>Walkway/Pedestrian Trail</p>
<p>S1720</p>
<p>Stairway</p>
<p>S1730</p>
<p>Alley</p>
<p>S1740</p>
<p>Private Road for service vehicles</p>
<p>S1750</p>
<p>Internal U.S. Census Bureau use</p>
<p>S1780</p>
<p>Parking Lot Road</p>
<p>S1820</p>
<p>Bike Path or Trail</p>
<p>S1830</p>
<p>Bridle Path</p>
<p>S2000</p>
<p>Road Median</p>
<p>Let’s say we want to create a map with rules based on the MTFCC values shown in Table F5.3.2.</p>
<p>Table F5.3.2&nbsp;Styling Parameters for Road Priority Codes</p>
<p>MTFCC</p>
<p>Color</p>
<p>Line Width</p>
<p>S1100</p>
<p>Blue</p>
<p>3</p>
<p>S1200</p>
<p>Green</p>
<p>2</p>
<p>S1400</p>
<p>Orange</p>
<p>1</p>
<p>All Other Classes</p>
<p>Gray</p>
<p>1</p>
<p>Let’s define a dictionary containing the styling information.</p>
<p>var&nbsp;styles = ee.Dictionary({&nbsp; &nbsp;‘S1100’: {&nbsp; &nbsp; &nbsp; &nbsp;‘color’: ‘blue’,&nbsp; &nbsp; &nbsp; &nbsp;‘width’: 3&nbsp; &nbsp;},&nbsp; &nbsp;‘S1200’: {&nbsp; &nbsp; &nbsp; &nbsp;‘color’: ‘green’,&nbsp; &nbsp; &nbsp; &nbsp;‘width’: 2&nbsp; &nbsp;},&nbsp; &nbsp;‘S1400’: {&nbsp; &nbsp; &nbsp; &nbsp;‘color’: ‘orange’,&nbsp; &nbsp; &nbsp; &nbsp;‘width’: 1&nbsp; &nbsp;}<br>
});var&nbsp;defaultStyle = {<br>
&nbsp; &nbsp;color: ‘gray’,&nbsp; &nbsp;‘width’: 1<br>
};</p>
<p>The style&nbsp;function needs a property in the FeatureCollection&nbsp;that contains a dictionary with the style parameters. This allows you to specify a different style for each feature. To create a new property, we map&nbsp;a function&nbsp;over the FeatureCollection&nbsp;and assign an appropriate style dictionary to a new property named ‘style’. Note the use of the get&nbsp;function, which allows us to fetch the value for a key in the dictionary. It also takes a default value in case the specified key does not exist. We make use of this to assign different styles to the three road&nbsp;classes specified in Table 5.3.2 and a default style to all others.</p>
<p>var&nbsp;sfRoads = sfRoads.map(function(f) {&nbsp; &nbsp;var&nbsp;classcode = f.get(‘mtfcc’);&nbsp; &nbsp;var&nbsp;style = styles.get(classcode, defaultStyle);&nbsp; &nbsp;return&nbsp;f.set(‘style’, style);<br>
});</p>
<p>Our collection is now ready to be styled. We call the&nbsp;style&nbsp;function to specify the property that contains the dictionary of style parameters. The output of the style&nbsp;function is an RGB image rendered from the FeatureCollection&nbsp;(Fig. F5.3.4).</p>
<p>var&nbsp;sfRoadsStyle = sfRoads.style({<br>
&nbsp; &nbsp;styleProperty: ‘style’<br>
});<br>
Map.addLayer(sfRoadsStyle.clip(geometry), {}, ‘Roads (Style)’);</p>
<p><img src="F5/image46.png" class="img-fluid"></p>
<p>Fig. F5.3.4&nbsp;San Francisco roads rendered according to road priority</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F53a.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.</p>
</div>
</div>
<p>Save your script for your own future use, as outlined in Chap. F1.0. Then, refresh the Code Editor to begin with a new script for the next section.</p>
</section>
</section>
<section id="joins-with-feature-collections" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="joins-with-feature-collections"><span class="header-section-number">10.2</span> Joins with Feature Collections</h2>
<p>Earth Engine was designed as a platform for processing raster data, and that is where it shines. Over the years, it has acquired advanced vector data processing capabilities, and users are now able to carry out complex geoprocessing tasks within Earth Engine. You can leverage the distributed processing power of Earth Engine to process large vector layers in parallel.</p>
<p>This section shows how you can do spatial queries and spatial joins using multiple large feature collections.&nbsp;This requires the use of joins. As described for Image Collections in Chap. F4.9, a join allows you to match every item in a collection with items in another collection based on certain conditions. While you can achieve similar results using map&nbsp;and filter, joins perform better and give you more flexibility. We need to define the following items to perform a join on two collections.</p>
<ol type="1">
<li>Filter:&nbsp;A filter defines the condition used to select the features from the two collections. There is a suite of filters in the ee.Filters&nbsp;module that work on two collections, such as ee.Filter.equals&nbsp;and&nbsp;ee.Filter.withinDistance.</li>
<li>Join type: While the filter determines which features will be joined, the join type determines how they will be joined. There are many join types,&nbsp;including simple join, inner join, and save-all join.</li>
</ol>
<p>Joins are one of the harder skills to master, but doing so will help you perform many complex analysis tasks within Earth Engine. We will go through practical examples that will help you understand these concepts and the workflow better.</p>
<section id="selecting-by-location" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="selecting-by-location"><span class="header-section-number">10.2.1</span> Selecting by Location</h3>
<p>In this section, we will learn how to select features from one layer that are within a specified distance from features in another layer. We will continue to work with the San Francisco census blocks and roads datasets from the previous section. We will implement a join to select all blocks in San Francisco that are within 1 km of an interstate highway.</p>
<p>We start by loading the census blocks and roads collections and filtering the roads layer to the San Francisco boundary.</p>
<p>var&nbsp;blocks = ee.FeatureCollection(‘TIGER/2010/Blocks’);<br>
var&nbsp;roads = ee.FeatureCollection(‘TIGER/2016/Roads’);<br>
var&nbsp;sfNeighborhoods = ee.FeatureCollection(&nbsp; &nbsp;‘projects/gee-book/assets/F5-0/SFneighborhoods’);</p>
<p>var&nbsp;geometry = sfNeighborhoods.geometry();<br>
Map.centerObject(geometry);</p>
<p>// Filter blocks and roads to San Francisco boundary.<br>
var&nbsp;sfBlocks = blocks.filter(ee.Filter.bounds(geometry));<br>
var&nbsp;sfRoads = roads.filter(ee.Filter.bounds(geometry));</p>
<p>As we want to select all blocks within 1 km of an interstate highway, we first filter the sfRoads&nbsp;collection to select all segments with the rttyp&nbsp;property value of I.</p>
<p>var&nbsp;interstateRoads = sfRoads.filter(ee.Filter.eq(‘rttyp’, ‘I’));</p>
<p>We use the draw&nbsp;function to visualize the sfBlocks&nbsp;and interstateRoads&nbsp;layers (Fig. F5.3.5).</p>
<p>var&nbsp;sfBlocksDrawn = sfBlocks.draw({<br>
&nbsp; &nbsp; &nbsp; &nbsp;color: ‘gray’,<br>
&nbsp; &nbsp; &nbsp; &nbsp;strokeWidth: 1&nbsp; &nbsp;})<br>
&nbsp; &nbsp;.clip(geometry);<br>
Map.addLayer(sfBlocksDrawn, {}, ‘All Blocks’);<br>
var&nbsp;interstateRoadsDrawn = interstateRoads.draw({<br>
&nbsp; &nbsp; &nbsp; &nbsp;color: ‘blue’,<br>
&nbsp; &nbsp; &nbsp; &nbsp;strokeWidth: 3&nbsp; &nbsp;})<br>
&nbsp; &nbsp;.clip(geometry);<br>
Map.addLayer(interstateRoadsDrawn, {}, ‘Interstate Roads’);</p>
<p><img src="F5/image2.png" class="img-fluid"></p>
<p>Fig. F5.3.5 San Francisco blocks and interstate highways</p>
<p>Let’s define a join that will select all the features from the sfBlocks&nbsp;layer that are within 1 km of any feature from the interstateRoads&nbsp;layer. We start by defining a filter using the ee.Filter.withinDistance&nbsp;filter. We want to compare the geometries of features in both layers, so we use a special property called ‘.geo’&nbsp;to compare the collections. By default, the filter will work with exact distances between the geometries. If your analysis does not require a very precise tolerance of spatial uncertainty, specifying a small non-zero maxError&nbsp;distance value will help speed up the spatial operations. A larger tolerance also helps when testing or debugging code so you can get the result quickly instead of waiting longer for a more precise output.</p>
<p>var&nbsp;joinFilter = ee.Filter.withinDistance({<br>
&nbsp; &nbsp;distance: 1000,<br>
&nbsp; &nbsp;leftField: ‘.geo’,<br>
&nbsp; &nbsp;rightField: ‘.geo’,<br>
&nbsp; &nbsp;maxError: 10<br>
});</p>
<p>We will use a simple join&nbsp;as we just want features from the first (primary) collection that match the features from the other (secondary) collection.</p>
<p>var&nbsp;closeBlocks = ee.Join.simple().apply({<br>
&nbsp; &nbsp;primary: sfBlocks,<br>
&nbsp; &nbsp;secondary: interstateRoads,<br>
&nbsp; &nbsp;condition: joinFilter<br>
});</p>
<p>We can visualize the results in a different color and verify that the join worked as expected (Fig. F5.3.6).</p>
<p>var&nbsp;closeBlocksDrawn = closeBlocks.draw({<br>
&nbsp; &nbsp; &nbsp; &nbsp;color: ‘orange’,<br>
&nbsp; &nbsp; &nbsp; &nbsp;strokeWidth: 1&nbsp; &nbsp;})<br>
&nbsp; &nbsp;.clip(geometry);<br>
Map.addLayer(closeBlocksDrawn, {}, ‘Blocks within 1km’);</p>
<p><img src="F5/image40.png" class="img-fluid"></p>
<p>Fig. F5.3.6&nbsp;Selected blocks within 1 km of an interstate highway</p>
</section>
<section id="spatial-joins" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="spatial-joins"><span class="header-section-number">10.2.2</span> Spatial Joins</h3>
<p>A spatial join&nbsp;allows you to query two collections based on the spatial relationship. We will now implement a spatial join to count points in polygons. We will work with a dataset of tree locations in San Francisco and polygons of neighborhoods to produce a CSV file with the total number of trees in each neighborhood.</p>
<p>The&nbsp;San Francisco Open Data Portal&nbsp;maintains a street tree map dataset that has a list of street trees with their latitude and longitude. We will also use the San Francisco neighborhood dataset from the same portal. We downloaded, processed, and uploaded these layers as Earth Engine assets for use in this exercise. We start by loading both layers and using the paint&nbsp;and style&nbsp;functions, covered in Sect. 1, to visualize them (Fig. F5.3.7).</p>
<p>var&nbsp;sfNeighborhoods = ee.FeatureCollection(&nbsp; &nbsp;‘projects/gee-book/assets/F5-0/SFneighborhoods’);<br>
var&nbsp;sfTrees = ee.FeatureCollection(&nbsp; &nbsp;‘projects/gee-book/assets/F5-3/SFTrees’);</p>
<p>// Use paint() to visualize the polygons with only outline<br>
var&nbsp;sfNeighborhoodsOutline = ee.Image().byte().paint({<br>
&nbsp; &nbsp;featureCollection: sfNeighborhoods,<br>
&nbsp; &nbsp;color: 1,<br>
&nbsp; &nbsp;width: 3<br>
});<br>
Map.addLayer(sfNeighborhoodsOutline, {<br>
&nbsp; &nbsp; &nbsp; &nbsp;palette: [‘blue’]<br>
&nbsp; &nbsp;},&nbsp; &nbsp;‘SF Neighborhoods’);</p>
<p>// Use style() to visualize the points<br>
var&nbsp;sfTreesStyled = sfTrees.style({<br>
&nbsp; &nbsp;color: ‘green’,<br>
&nbsp; &nbsp;pointSize: 2,<br>
&nbsp; &nbsp;pointShape: ‘triangle’,<br>
&nbsp; &nbsp;width: 2<br>
});<br>
Map.addLayer(sfTreesStyled, {}, ‘SF Trees’);</p>
<p><img src="F5/image35.png" class="img-fluid"></p>
<p>Fig. F5.3.7&nbsp;San Francisco neighborhoods and trees</p>
<p>To find the tree points in each neighborhood polygon, we will use an ee.Filter.intersects&nbsp;filter.</p>
<p>var&nbsp;intersectFilter = ee.Filter.intersects({<br>
&nbsp; &nbsp;leftField: ‘.geo’,<br>
&nbsp; &nbsp;rightField: ‘.geo’,<br>
&nbsp; &nbsp;maxError: 10<br>
});</p>
<p>We need a join that can give us a list of all tree features that intersect each neighborhood polygon, so we need to use a saving join. A saving join will find all the features from the secondary collection that match the filter and store them in a property in the primary collection. Once you apply this join, you will get a version of the primary collection with an additional property that has the matching features from the secondary collection. Here we use the ee.Join.saveAll&nbsp;join, since we want to store all matching features. We specify the matchesKey&nbsp;property that will be added to each feature with the results.</p>
<p>var&nbsp;saveAllJoin = ee.Join.saveAll({<br>
&nbsp; &nbsp;matchesKey: ‘trees’,<br>
});</p>
<p>Let’s apply the join and print the first feature of the resulting collection to verify (Fig. F5.3.8).</p>
<p>var&nbsp;joined = saveAllJoin<br>
&nbsp; &nbsp;.apply(sfNeighborhoods, sfTrees, intersectFilter);<br>
print(joined.first());</p>
<p><img src="F5/image1.png" class="img-fluid"></p>
<p>Fig. F5.3.8&nbsp;Result of the save-all join</p>
<p>You will see that each feature of the sfNeighborhoods&nbsp;collection now has an additional property called trees.&nbsp;This contains all the features from the sfTrees&nbsp;collection that were matched using the intersectFilter. We can now map&nbsp;a function over the results and post-process the collection. As our analysis requires the computation of the total number of trees in each neighborhood, we extract the matching features and use the size&nbsp;function to get the count (Fig. F5.3.9).</p>
<p>// Calculate total number of trees within each feature.<br>
var&nbsp;sfNeighborhoods = joined.map(function(f) {&nbsp; &nbsp;var&nbsp;treesWithin = ee.List(f.get(‘trees’));&nbsp; &nbsp;var&nbsp;totalTrees = ee.FeatureCollection(treesWithin).size();&nbsp; &nbsp;return&nbsp;f.set(‘total_trees’, totalTrees);<br>
});</p>
<p>print(sfNeighborhoods.first());</p>
<p><img src="F5/image18.png" class="img-fluid"></p>
<p>Fig. F5.3.9&nbsp;Final FeatureCollection&nbsp;with the new property</p>
<p>The results now have a property called total_trees&nbsp;containing the count of intersecting trees in each neighborhood polygon.</p>
<p>The final step in the analysis is to export the results as a CSV file using the Export.table.toDrive&nbsp;function. Note that as described in detail in F6.2, you should output only the columns you need to the CSV file. Suppose we do not need all the properties to appear in the output; imagine that wedo not&nbsp;need the trees&nbsp;property, for example, in the output. In that case, we can create only those columns we want in the manner below, by specifying the other selectors&nbsp;parameters with the list of properties to export.</p>
<p>// Export the results as a CSV.<br>
Export.table.toDrive({<br>
&nbsp; &nbsp;collection: sfNeighborhoods,<br>
&nbsp; &nbsp;description: ‘SF_Neighborhood_Tree_Count’,<br>
&nbsp; &nbsp;folder: ‘earthengine’,<br>
&nbsp; &nbsp;fileNamePrefix: ‘tree_count’,<br>
&nbsp; &nbsp;fileFormat: ‘CSV’,<br>
&nbsp; &nbsp;selectors: [‘nhood’, ‘total_trees’]<br>
});</p>
<p>The final result is a CSV file with the neighborhood names and total numbers of trees counted using the join (Fig. F5.3.10).</p>
<p><img src="F5/image3.png" class="img-fluid"></p>
<p>Fig. F5.3.10&nbsp;Exported CSV file with tree counts for San Francisco neighborhoods</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Code Checkpoint F53b.&nbsp;The book’s repository contains a script that shows what your code should look like at this point.</p>
</div>
</div>
</section>
</section>
<section id="synthesis-3" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="synthesis-3">Synthesis</h2>
<p>Assignment 1.&nbsp;What join would you use if you wanted to know which neighborhood each tree belongs to? Modify the code above to do a join and post-process the result to add a neighborhood property to each tree point. Export the results as a shapefile.</p>
</section>
<section id="conclusion-3" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="conclusion-3">Conclusion</h2>
<p>This chapter covered visualization and analysis using vector data in Earth Engine. You should now understand different functions for FeatureCollection&nbsp;visualization and be able to create thematic maps with vector layers. You also learned techniques for doing spatial queries and spatial joins within Earth Engine. Earth Engine is capable of handling large feature collections and can be effectively used for many spatial analysis tasks.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./F4.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Interpreting Image Series</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./F6.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advanced Topics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>